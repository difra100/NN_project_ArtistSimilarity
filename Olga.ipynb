{"cells":[{"cell_type":"markdown","source":["# Neural Network project, 2022\n","## - Artist similarity with Graph Neural Networks (re-implementation)\n","- Andrea Giuseppe Di Francesco, 1836928\n","- Giuliano Giampietro, 2024160\n","\n","* In this notebook we present a complete re-implementation of the paper ['Artist similarity with Graph Neural Network'](https://arxiv.org/abs/2107.14541). \n","* Since the project's code wasn't provided by the authors, except for the [dataset](https://gitlab.com/fdlm/olga://paperswithcode.com/paper/artist-similarity-with-graph-neural-networks), we attempted to repeat the experiments described in the paper, and we have additionally tried 4 additional GNNs architectures, provided by the Phd student Indro Spinelli.\n"],"metadata":{"id":"4fmrN5VAWUrJ"},"id":"4fmrN5VAWUrJ"},{"cell_type":"markdown","source":["## - Importing the libraries\n","\n","In the following cell we import the libraries that we used to carry out our experiments, and to extract the dataset. Since we worked with the Graph Neural Networks (GNN), it was very helpful to use the [pytorch geometric library](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html), that contains a lot of useful classes that implement the most famous GNNs architectures."],"metadata":{"id":"serwR-KUl2Ym"},"id":"serwR-KUl2Ym"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9e657a64","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652956381051,"user_tz":-120,"elapsed":40325,"user":{"displayName":"Giuliano Giampietro","userId":"16624639823769972529"}},"outputId":"742cb6d9-da68-4e9c-dfc9-2af1ae5c1202"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n","\u001b[K     |████████████████████████████████| 7.9 MB 2.7 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.13\n","Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 2.5 MB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.0\n","Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n","\u001b[K     |████████████████████████████████| 750 kB 2.7 MB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n","\u001b[K     |████████████████████████████████| 407 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=28c0d5ab7b5e43d36dae4d05a5f7ea3746636d3255175c896254e4049204c7c6\n","  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n","Successfully built torch-geometric\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.0.4\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7ff26ef044f0>"]},"metadata":{},"execution_count":1}],"source":["!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","!pip install torch-geometric\n","import pandas as pd\n","import requests\n","# !pip install requests_html\n","# from requests_html import HTMLSession\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import json\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler\n","import random\n","from random import choice,randrange\n","import matplotlib.pyplot as plt\n","from sklearn.neighbors import NearestNeighbors\n","import math\n","import time\n","from torch_geometric.nn import GCNConv,GraphConv,GATConv,SAGEConv\n","\n","random_seed=80085\n","\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)"],"id":"9e657a64"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1929,"status":"ok","timestamp":1652957023316,"user":{"displayName":"Giuliano Giampietro","userId":"16624639823769972529"},"user_tz":-120},"id":"2gTmSZCZ7QvN","outputId":"da595961-b8f8-4b6b-b5fe-8dcaa0247e51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["## If executing this cell in google drive, run this cell ##\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"id":"2gTmSZCZ7QvN"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1652956381051,"user":{"displayName":"Giuliano Giampietro","userId":"16624639823769972529"},"user_tz":-120},"id":"QUdMbad37Zat","outputId":"9d9bcdfc-ab70-4f72-aff5-4efb01344210"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/My Drive/NNProject/papers/NNArtistSimilarity'\n","/content\n"]}],"source":["## Selection of the current directory ##\n","%cd drive/My Drive/NNProject/papers/NNArtistSimilarity"],"id":"QUdMbad37Zat"},{"cell_type":"markdown","metadata":{"id":"cbd20535"},"source":["## Loading of the dataset Olga\n","\n"," - In the next cell there is the raw dataset provided by the paper's authors **'Artist similarity with Graph Neural Networks'**.\n"," - Along the columns we have information about the [Musicbrainz_id](https://musicbrainz.org/) of an artist, its partition in the dataset (train,val,test), and the [AcousticBrainz](https://acousticbrainz.org/) low level features of the artist, taken from a sample of 25 songs. \n"," - Unfortunately, at the best of our efforts, it was not possible to recover all the information that were described in the [paper](https://arxiv.org/pdf/2107.14541.pdf). Indeed the artists contained in Olga are 17.673, whereas we were able to extract [Allmusic](https://www.allmusic.com/) ids only from 11.261 artists. \n","\n"],"id":"cbd20535"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"executionInfo":{"elapsed":256,"status":"error","timestamp":1652957148944,"user":{"displayName":"Giuliano Giampietro","userId":"16624639823769972529"},"user_tz":-120},"id":"9c407599","outputId":"813fefbb-66ea-4db3-edd1-8dafd39c242c"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-348b1b14f872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0molga\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'olga.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#train 0-14138, #val 14139-15905, #test 15906-17673 (indices)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0molga\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'olga.csv'"]}],"source":["olga=pd.read_csv('olga.csv')\n","#train 0-14138, #val 14139-15905, #test 15906-17673 (indices)\n","olga.head()  "],"id":"9c407599"},{"cell_type":"markdown","metadata":{"id":"08648966"},"source":["### How do we retrieve the Graph topology?\n","\n","- Thanks to the musicbrainz_id of each artist, we can get the link to its AllMusic profile, and from there we get also the information about its related artists. Each AllMusic link is related to a unique artist, indeed we can spot a 12 numbers identifier for each of these.\n","- After having obtained the AllMusic link for each artist in the dataset (if exists), we want to associate to each artist its related ones. We do this just for those that can be re-mapped in the dataset's musicbrainz_ids, because we have associated tracks features for those.\n","- Also this passage is probably very lossy, in fact, for each artist there could be a lot of similar artists (according to the AllMusic information), but we don't have the feature vectors for all of them.\n","- The following class contains methods that extract information about all the artists in the dataset, but the price to pay is a high computational cost. \n","For this reason it was run just once, and the information was stored in different files, such as 'MsbMapped.json', 'graphSimilarities.json', 'dizofartist.json'."],"id":"08648966"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3712d1b8","scrolled":false},"outputs":[],"source":["class DatasetOlga(): #In this class, we obtain through different methods the main characteristics of the graph of artists\n","                    # thanks to the available information in the olga dataset\n","    def __init__(self,olga):\n","        self.olga=olga\n","        self.mb=olga.musicbrainz_id\n","        self.artists={} #Needed for obtaining the mapping from musicbrainz to the allmusic ids\n","        self.l=len(self.mb)\n","        self.d={}       #Needed for obtaining a dict. where keys are artists, and values are the artists similar to them, based on self.artists\n","        self.NI={}      #Dict. that will contain the artist's features\n","    \n","    def get_mapping(self,i):                                                                                                 #This method returns the allmusic page of an artist (if exists), given his id from the dataset \n","        response = requests.get(f'https://musicbrainz.org/ws/2/artist/{str(self.mb[i])}?inc=url-rels&fmt=json')\n","        if response.ok:\n","            data = response.json()\n","            refs = [r['url']['resource'] for r in data['relations'] if r['type'] == 'allmusic']        \n","            return refs[0] if len(refs) != 0 else \"Not found\"\n","\n","    def get_artist_name(self,i):                                                                                                 #This method returns the allmusic page of an artist (if exists), given his id from the dataset \n","        response = requests.get(f'https://musicbrainz.org/ws/2/artist/{str(self.mb[i])}?inc=url-rels&fmt=json')\n","        if response.ok:\n","            data = response.json()\n","            data = dict(data)\n","            return data['name']\n","\n","\n","\n","    def get_mappingList(self,init,end,increm=500):\n","        Lmusicbrainz_id=self.mb[init:end] #We can specify the range of the artists of our interest, for the purpose of this NN task\n","        length=len(Lmusicbrainz_id)       #we will take all of them into consideration.\n","        c=0\n","        for i in range(len(Lmusicbrainz_id)):\n","            mapp=self.get_mapping(i)   #get_mapping method again.\n","            if mapp==None:\n","                while mapp==None:\n","                    mapp=self.get_mapping(i)\n","                    \n","            if mapp!=\"Not found\":   #Some of the ids has not a respective allmusic id, so we lose that information\n","                mapp=str(mapp)      #Mapp are strings of links\n","                key=mapp[-12:]\n","                self.artists[key]=i\n","            c+=1\n","            if c%increm==0 or c==30:\n","                    print(\"{}/{} artists were processed\".format(c,length)) #This is just to keep track of the processed artist\n","                    \n","            \n","        self.save_data(self.artists,'MsbMapped1.json')  #We do save the Artists Ids map, this function, when called, takes a lot\n","                                                        #of time, for this reason its result is already saved in the file:\n","        return self.artists                             # 'MsbMapped1.json'\n","    \n","    \n","    def get_GraphDict(self,name='MsbMapped1.json',increm=500):\n","        session=HTMLSession()\n","        c=0 #Counter\n","        artID=self.load_data(name) #We load the mapped artists (between MusicBrainz Ids, and AllMusic Ids)\n","        length=len(artID.keys())\n","        for k in artID.keys(): #dict of mapped mbids, this has to be computed before from getmapping\n","            if k!=None:\n","                url='https://www.allmusic.com/artist/'+ k+ '/related' #k is just the code, every link for the artist is distinguished \n","                r=session.get(url)                                    #by a unique code in the link.\n","                sess=r.html.find('body',first=True)\n","                div=sess.find('.overflow-container')                  #The information of the related artists are exctracted\n","                divn=div[0]                                           #from the html of the allmusic's related web page\n","                divn=divn.find('.content-container')\n","                divn=divn[0]\n","                divn=divn.find('.content')\n","                divn=divn[0]\n","                divn=divn.find('section',first=True)\n","                if divn==None:\n","                    self.d[artID[k]]=[] #That artist has not related artists (or we have missing information)\n","                    continue\n","                artists=divn.find('li')\n","                artistL=[]\n","\n","\n","                for i in range(len(artists)):\n","                    art=artists[i]\n","                    art=art.find('a')            #We look for all the k's related artists links\n","                    link=list(art[0].absolute_links)[0] #Absolute_link returns a one-element set, that we convert into a list and\n","                    link=str(link)[-12:]                #we get its code\n","                    if link in artID.keys(): #g is the dict of all the mapped musicbrainz_ids\n","                        artistL.append(self.artists[link]) #Some of the related artists may not be in the musicbrainz_ids list.\n","                self.d[artID[k]]=artistL\n","                c+=1\n","                if c%increm==0 or c==30:\n","                    print(\"{}/{} artists were processed\".format(c,length))\n","        self.save_data(self.d,'graphSimilarities1.json') #Here we save the connection amongst the artists, obtained with this method\n","        print(\"Done...\")     #Also it takes some time to process, for this reason the result of this method can be \n","        return self.d        #found at the 'graphSimilarities.json' file.\n","    \n","    def save_data(self,dicti,name):\n","        jfile = open(name, \"w\")\n","        jfile = json.dump(dicti, jfile)\n","    \n","    def load_data(self,name):\n","        jfile = open(name, \"r\")\n","        dicti = json.load(jfile)\n","        return dicti\n","ol = DatasetOlga(olga)"],"id":"3712d1b8"},{"cell_type":"markdown","metadata":{"id":"13f2be61"},"source":["## Graph construction\n","\n","- Once we have obtained the information necessary to construct the Graph topology, and stored them into two json files ('MsbMapped.json','graphSimilarities.json'), we still need to have the Graph data structure to feed the Graph Convolutional Network.\n","\n","- In the following cell are defined the graph's adjacency matrix from the 'graphSimilarities.json', which was previously obtained, and the features of each artist, namely the attributes of the graph's nodes. Those are obtained from a numpy array stored in the file 'acousticbrainz.npy', such file was provided by the paper's authors."],"id":"13f2be61"},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":297,"status":"error","timestamp":1652956920067,"user":{"displayName":"Giuliano Giampietro","userId":"16624639823769972529"},"user_tz":-120},"id":"145ae3a2","scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":380},"outputId":"c6e2cde5-76d3-47e1-a28e-ba7b0335da3a"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d627ef115794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdicti\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MsbMapped.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'graphSimilarities.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acousticbrainz.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adjacency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymmetry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-d627ef115794>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mapfile, gfile)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmapfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#The expected files are the ones mentioned before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetOlga\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0molga\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-d627ef115794>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mjfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mdicti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdicti\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MsbMapped.json'"]}],"source":["n_features=2613\n","class Graph():  #The purpose of this class is to construct the graph of artists, in particular the Adjacency matrix A, and the  \n","                # node features tensor X\n","        \n","    def __init__(self,mapfile,gfile):  #The expected files are the ones mentioned before.\n","        self.mfile=self.load_data(mapfile)\n","        self.gfile=self.load_data(gfile)\n","        self.ol = DatasetOlga(olga)\n","        self.A=torch.zeros((len(self.mfile),len(self.mfile)))\n","        self.X=torch.zeros((n_features,len(self.mfile)))\n","        self.ord=sorted(list(map(int,self.gfile.keys())))\n","        self.enc1={}\n","        self.enc2={}\n","    \n","    #With the preprocessing step at the previous cell we have lost some information\n","    #and also the ordering of the artists, so i have defined a method that for each previous artist index\n","    #we can encode it to a new ordered list of artists.\n","    \n","    \n","    def encoding1(self):   #From ordered to unordered, Dict are not ordered data structures, so is better to order them before\n","        for k in range(len(self.mfile)): #This encoding is used to get the Instance matrix\n","            self.enc1[k]=self.ord[k]\n","        return self.enc1\n","    \n","    def encoding2(self):   #From unordered to ordered,  From real number, to ordered one.\n","        for k in range(len(self.mfile)): #This encoding is used to get the Adjacency matrix\n","            self.enc2[self.ord[k]]=k\n","        return self.enc2\n","    \n","    def get_instance(self,instances,df=False):#We take the features centroid, obtained from 25 track from artists discographies.\n","        X=np.load(instances)                  #The instances file is provided by the repository mentioned in the paper.\n","        X=torch.from_numpy(X).requires_grad_(True) #We take the allmusicIDs, which contain the key of the artists for which we haven't \n","        c=0                                        # lost information\n","        enc=self.encoding1()\n","        for k in self.mfile:\n","            z=enc[c]\n","            self.X[:,c]=X[z] \n","            c+=1\n","        return self.X\n","    \n","    def get_adjacency(self,symmetry=False,df=False):  #The hypothesis could be either a symmetric matrix (paper), or not.\n","        enc=self.encoding2()\n","        for k in self.gfile:\n","            c1=enc[int(k)]\n","            for j in self.gfile[k]:\n","                c2=enc[int(j)]\n","                if self.A[c2,c1]==1 and symmetry==True:\n","                    continue\n","                self.A[c1,c2]=1\n","                if symmetry:\n","                    self.A[c2,c1]=1\n","\n","            \n","        return self.A\n","    def get_artist_dict(self):\n","      self.id_to_art = {}\n","      for key in self.enc1:\n","        self.id_to_art[key] = self.ol.get_artist_name(self.enc1[key])\n","        print(key)\n","\n","\n","    def load_data(self,name):\n","        jfile = open(name, \"r\")\n","        dicti = json.load(jfile)\n","        return dicti\n","    \n","g=Graph('MsbMapped.json','graphSimilarities.json')\n","X1=g.get_instance('acousticbrainz.npy')\n","A1=g.get_adjacency(symmetry=True)\n","\n","## diz_of_artist contains a mapping from the ids to the artists ##\n","diz_of_artist = ol.load_data('dizofartist.json')\n","\n","## art_to_code is the opposite of diz_of_artist ##\n","art_to_code = {diz_of_artist[key]:key for key in diz_of_artist}\n","## The artists names are used to see how much the different artists are distant, and also to see what are their actual names, instead of their vectors."],"id":"145ae3a2"},{"cell_type":"markdown","metadata":{"id":"e080eafd"},"source":["# GraphSAGE model\n","\n","- Once we also have the features for each instance, and the adjacency matrix of the graph, we can start to design the Graph Convolutional Layers, and the Fully Connected layers, as described in the paper.\n","\n","- Every feature vector has 2613 elements (low level features of the artists), our aim is to embed these vectors in a 100-dimensional space, where the distance between its points (Euclidean distance) represents a musical distance among the artists. (It is not clear however what each dimension stands for).\n","The Graph Neural network attempts to learn this embedded space.\n","\n","- The GraphNN that the paper's authors decided to use is the GraphSAGE model (SAGE stands for Sample and AGgregatE).\n","\n","\n","\n","- This approach proposes a framework that generalizes the GCN to use trainable aggregation functions (beyond simple convolutions). ![Alt text](data:image/png;base64,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 \"GraphSAGE structure\")\n","- This type of approach was introduced in the paper ['Inductive Representation Learning on Large Graphs'](https://arxiv.org/abs/1706.02216), that is also where it was taken the previous image."],"id":"e080eafd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1121362"},"outputs":[],"source":["class GraphSAGE(nn.Module):\n","    \n","    def __init__(self,X,A,train_set,test_set,L,batch_size,device, training_mode = True):\n","        \n","        super(GraphSAGE,self).__init__()\n","        self.device=device\n","        self.A=A.to(self.device) #Tensors version of adjacency matrix and Instances\n","        ## There are two types of adjacency matrix that could be useful ##\n","        self.Atrain=self.select(self.A,train_set,train_set).to(self.device)\n","        self.ATot=self.select(self.A,train_set+test_set,train_set+test_set).to(self.device)\n","        \n","        self.feat=100\n","        self.X=torch.tensor(X.tolist(),requires_grad=True).to(self.device)   \n","        self.COO=torch.load('COOA.pt') #This file contains the whole adjacency matrix in coordinate format (which is the torch geometric format)\n","        self.train=train_set\n","        self.test=test_set\n","        self.L=L   #Number of layers\n","        self.lamb=0.8  #Percentage used for the weighted distance to choose the hard positives and the hard negatives.\n","        if training_mode:  #When we train we use this setting, otherwise is faster to set training_mode to false, in the case we want to carry out a forward on a batch of samples. \n","          self.bs=batch_size\n","          self.diz=self.getDiz()\n","          self.mb=self.mini_batches(self.train+self.test,self.bs)\n","          self.OrDiz=self.getCorrenspondancies(self.mb)\n","        self.SG1=SAGEConv(2613,256,normalize=True,aggr=\"mean\")\n","        self.SG2=SAGEConv(256,256,normalize=True,aggr=\"mean\")\n","        self.SG3=SAGEConv(256,256,normalize=True,aggr=\"mean\")\n","        \n","        self.FC1=nn.Linear(256,256)\n","        self.FC2=nn.Linear(256,256)\n","        self.FC4=nn.Linear(256,256)\n","        self.FC3=nn.Linear(256,self.feat)\n","        \n","        \n","    def forward(self,V,b,nbs=-1,eval_mode = False):\n","        Vdiz=self.tracing(V,b, eval_mode)\n","        \n","        # If these statements hold it means that we have already defined the mini batches for the training data, and we don't have to look for their \n","        # neighbors everytime. Otherwise we get the neighbors for them.\n","        if len(V)>500 and nbs!=-1:\n","            OrDiz=self.OrDiz[nbs]\n","        else:\n","            OrDiz=self.getCorr(Vdiz) #Every sample/artist is normalized from 0 to 512(batch_size), in order to get easily the indices from the matrix A.\n","            \n","        for k in range(0,self.L):  #k reach maximum 2, the precise number depends on the number of layers that we decide to use for the GraphSAGE's models\n","            \n","\n","            if k==0:\n","                Es=self.select(self.X,set(),Vdiz[k+1]).T\n","                Anew=self.select(self.A,Vdiz[k+1],Vdiz[k+1])\n","                Anew=self.ConvertAtoCOO(Anew)\n","                Es=self.SG1(Es,Anew).T\n","                Es=F.elu(Es)\n","    \n","            if k==1:\n","                Es=self.select(Es,set(),OrDiz[k+1].keys()).T\n","                Anew=self.select(self.A,Vdiz[k+1],Vdiz[k+1])\n","                Anew=self.ConvertAtoCOO(Anew)\n","                Es=self.SG2(Es,Anew).T\n","                Es=F.elu(Es)\n","                \n","            \n","            if k==2:\n","                Es=self.select(Es,set(),OrDiz[k+1].keys()).T\n","                Anew=self.select(self.A,Vdiz[k+1],Vdiz[k+1])\n","                Anew=self.ConvertAtoCOO(Anew)\n","                Es=self.SG3(Es,Anew).T\n","                Es=F.elu(Es)\n","                \n","            \n","            if k==self.L-1:\n","                Es=self.select(Es,set(),OrDiz[k+2]).T\n","            \n","            \n","        out=self.FC1(Es)\n","        out=self.FC2(out)\n","        out=self.FC3(out)\n","        return out.T\n","    \n","    def getCorrenspondancies(self,mbb):  ## If we are training the normalization of the batch samples is done from the pre-computed mini-batches ##\n","        OrDiz={}\n","        num=int(len(self.train)/self.bs)+1\n","        for j in range(len(mbb[:num])):\n","          Vdiz=self.tracing(mbb[j],1)\n","          OrDiz[j]={}\n","          for k in Vdiz:\n","              OrDiz[j][k]={}\n","              OrDiz[j][k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","          print(\"Processed {}-th mini-batch\".format(j+1))\n","        return OrDiz\n","    \n","    def getCorr(self,Vdiz): \n","        OrDiz={}\n","        for k in Vdiz:\n","            OrDiz[k]={}\n","            OrDiz[k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","        return OrDiz\n","            \n","            \n","    \n","    def tracing(self,V,b,eval_mode = False):  # To perform the FFW in the Graph Networks we need to trace the neighbors for each samples. \n","        Vdiz={}                               # This is done through this method and with the 'get_n' method\n","        K=self.L+1\n","        Vdiz[K]=sorted(list(V))\n","        for k in range(K-1,0,-1):\n","            d=set()\n","            for idx in Vdiz[k+1]: \n","                d=d.union(self.get_n(idx,b, eval_mode))\n","                \n","            Vdiz[k]=d\n","        return Vdiz\n","            \n","    def get_n(self,idx,b, eval_mode): #This function is the neighbor's function. Given a batch index we get its neighborhood.\n","        if b==1:\n","            A=self.Atrain\n","        else:\n","            A=self.ATot\n","        t=torch.nonzero(A[idx])\n","        s=set()\n","        \n","        for k in t:\n","            if t.shape[0]!=0 and eval_mode == False:\n","              s.add(k.item())\n","            \n","            elif eval_mode == True:\n","              if idx in self.test:\n","                if k.item() not in self.test:\n","                  s.add(k.item())\n","              else:\n","                s.add(k.item())\n","            \n","            else:\n","              continue\n","        s.add(idx)     \n","        return s\n","    \n","    def select(self,mat,row,col):  #Given a set of indices for rows or column or both, we get the respective elements.\n","        col=sorted(list(col))      #This is applied when we get the t matrix.\n","       \n","        c=0\n","        if row==set():\n","            col=torch.tensor(col,dtype=torch.int32).to(self.device)\n","            ma=torch.index_select(mat.to(self.device),1,col)\n","            return ma\n","        else:\n","            row=torch.tensor(sorted(list(row))).to(self.device)\n","            col=torch.tensor(col).to(self.device)\n","            ma=torch.index_select(mat,0,row)\n","            ma=torch.index_select(ma,1,col)\n","            return ma\n","    \n","   \n","    def getDiz(self):   # This method is used to get the positives and negatives for each samples, and eventually it is helpful to track them inside the mini-batches,\n","        diz={}          # thanks to the previously described methods.\n","        for k in range(self.COO.shape[1]):\n","            cn=int(self.COO[0][k].item())\n","            # if cn>self.train[-1]:\n","            #     break\n","            \n","            if cn not in diz:\n","                diz[cn]=[[int(self.COO[1][k])]]\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn].append([r])\n","            elif cn in diz:\n","                diz[cn][0].append(int(self.COO[1][k]))\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn][1].append(r)\n","                \n","        return diz\n","\n","    def ConvertAtoCOO(self,SA): # This function easily converts the adjacency matrix format to the coordinate matrix format, which is the format used in torch-geometric.\n","        Anew=torch.nonzero(SA).T.type(torch.LongTensor)\n","        return Anew.to(self.device)\n","\n","    def getHardP_N(self,currentB,tensor): # This method returns for a given batch its respective positives and negatives.\n","      samples=self.mbbPosNeg[currentB]\n","      positives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      negatives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      mbbList=sorted(list(self.mb[currentB]))\n","      for k in samples:\n","        pos=samples[k][0]\n","        neg=samples[k][1]\n","        distDizP={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in pos}\n","        distDizN={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in neg}\n","        summP=sum(list(distDizP.values()))\n","        summN=sum(list(distDizN.values()))\n","        if len(distDizP)!=0 and summP!=0:\n","          distDizP={kdist:distDizP[kdist]/summP for kdist in distDizP}\n","        else:\n","          distDizP[k]=0.5\n","        if len(distDizN)!=0 and summN!=0:\n","          distDizN={kdist:distDizN[kdist]/summN for kdist in distDizN}\n","        else:\n","          distDizN[k]=0.5\n","        keysP=list(distDizP.keys())\n","        keysN=list(distDizN.keys())\n","        \n","        for dist in keysP:\n","          if distDizP[dist]>=self.lamb and len(distDizP)!=1:\n","            distDizP.pop(dist)\n","        \n","        for dist in keysN:\n","          if distDizN[dist]<= 1-self.lamb and len(distDizN)!=1:\n","            distDizN.pop(dist)\n","        \n","        \n","        positives[mbbList.index(k)]=tensor[mbbList.index(max(distDizP,key=distDizP.get))]\n","        negatives[mbbList.index(k)]=tensor[mbbList.index(min(distDizN,key=distDizN.get))]\n","      \n","      return positives,negatives\n","        \n","        \n","    \n","    \n","    def mini_batches(self,indices,bs=32): #This function generates a list of minibatches, of size bs\n","        indexN=indices.copy()           #sets are unordered data structure, so there is no need to shuffle them. \n","        indicesTot=[indexN[:len(self.train)],indexN[self.test[0]:]] #In this method we also take the possible positive and negative for each of the sample.\n","        mbList=[]\n","        self.mbbPosNeg={} \n","        c=0   \n","        num=int(len(self.train)/self.bs)+1     #Lists of lists of mini_batches indices \n","        for indicesN in indicesTot:\n","          u=0\n","          if len(indicesN)==len(indexN[self.test[0]:]):\n","            mb=set(indicesN)\n","            mbList.append(mb)\n","            self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","            update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","            self.mbbPosNeg[c].update(update)\n","            return mbList\n","          while len(indicesN)!=0:\n","              mb=set()\n","                                  #Inner list, with the indices of a particular mini_batch\n","              while len(mb)<bs:\n","                  if len(indicesN)==0:\n","                      mbList.append(mb)\n","                      self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                      update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                      self.mbbPosNeg[c].update(update)\n","                      u=1\n","                      c+=1\n","                      break\n","\n","                  r=choice(indicesN)\n","                  sample=indicesN.pop(indicesN.index(r))\n","                  mb.add(sample)\n","              if u!=1:\n","                self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                self.mbbPosNeg[c].update(update)\n","                mbList.append(mb)\n","                c+=1\n","        return mbList          #obj.mini_batches(#,bs=128) #: train_,train,val,test, we get lists of list of batches from here\n","    \n","    \n","    def getNegSample(self,key,mb): ## This method looks for the closest 4 negatives in a batch, for each samples in it.\n","      l=[]\n","      for j in mb:\n","        if j!=key:\n","          if key not in self.diz:\n","            l.append(j)\n","            if len(l)==4:\n","              return l\n","          else:\n","            if j not in self.diz[key][0]:\n","              l.append(j)\n","              if len(l)==4:\n","                return l  \n","\n","\n","\n","    def calcG(self,ID):  #This method is used for the evaluation of accuracy, in particular it computes the denominator\n","        if ID>200:       # as described in the paper.\n","            ID=200\n","        c=1\n","        somm=0\n","        while c<=ID:\n","            somm+=1/(math.log2(1+c))\n","            c+=1\n","        return somm\n","\n","    def evalAcc(self,S,kneigh,b):  #This function is to compute accuracy for the test and train set. \n","        S = sorted(S)\n","        S1 = sorted(self.train + S)\n","        \n","        T = self.forward(S1,b, nbs = -1, eval_mode = True)\n","\n","        test_embs = self.select(T,set(),S).to(torch.device('cpu')).detach().T.numpy()\n","        T=T.detach().to(torch.device(\"cpu\")).numpy().transpose()\n","        \n","        neigh=NearestNeighbors(n_neighbors=(kneigh+1),algorithm='ball_tree').fit(test_embs) #With the K-NN we get the nearest \n","        \n","        dist,ind=neigh.kneighbors(test_embs)   \n","        \n","        acc=[]\n","        A_acc=self.select(self.A,S,S)\n","\n","        c=0\n","        for k in S:\n","            summ=0\n","            \n","            ideal = A_acc[ind[c][0]].sum().item()\n","            \n","            \n","            \n","            den=self.calcG(ideal)\n","            if den==0:\n","                c+=1\n","                continue  \n","            for j in range(len(ind[c][1:])):\n","                if A_acc[ind[c][0]][ind[c][1:][j]].item()!=0:\n","                    summ+= 1/(math.log2(1+(j+1)))\n","                    \n","                else:\n","                    continue\n","            c+=1    \n","            summ/=den\n","            acc.append(summ)\n","        return acc\n","    \n","    "],"id":"a1121362"},{"cell_type":"markdown","source":["## Utilities\n","* In the next cell are defined specific functions for our task.\n","* 'TrainingPipeLine' is simply the pipeline of our training, and it comprises also the computation of the Normalized Discounted Cumulative Gain (nDCG).\n","*There are also functions that are used laterly the notebook, such as 'Save_Model', 'Load_Model', 'plot_arrays', 'plot_metrics', 'get_accuracy', 'get_embeddings', 'get_nearest_artists'."],"metadata":{"id":"zgzkf8087Heo"},"id":"zgzkf8087Heo"},{"cell_type":"code","execution_count":51,"metadata":{"id":"5e580b5e","scrolled":false,"executionInfo":{"status":"ok","timestamp":1652961308420,"user_tz":-120,"elapsed":1458,"user":{"displayName":"andrea giuseppe di francesco","userId":"08400322680190058161"}}},"outputs":[],"source":["def TrainingPipeLine(model, optimizer, scheduler,loss, training, testing, num_epochs,KNN):\n","    start=time.time()\n","    bt=1 # Considers only the training samples from the adjacency matrix.\n","    btot=2 # Considers either the training and either the testing samples from the asjacency matrix.\n","    history_lossTr={}\n","    history_lossTe={}\n","    history_accuracy={}\n","    mbb=model.mb\n","    for epoch in range(num_epochs):\n","        print(\"Processing epoch n°\",epoch+1)\n","        num=int(len(training)/batch_size)+1\n","        startEP=time.time()\n","        history_lossTr[epoch+1]=[]\n","        for k in range(len(mbb[:num])):\n","            \n","            print(\"Processing {}-th epoch: {}/{} mini-batch\".format(epoch+1,k+1,num))\n","          \n","\n","            Ex=model(mbb[k],bt,nbs=k)\n","            anchors=Ex.T\n","            currentB=k\n","            positives,negatives=model.getHardP_N(currentB,anchors)\n","\n","            optimizer.zero_grad()\n","            lossTr=loss(anchors,positives.detach().to(device),negatives.detach().to(device))\n","            lossTr.backward()\n","            optimizer.step()\n","            \n","            history_lossTr[epoch+1].append(lossTr.item())\n","        \n","        \n","        with torch.no_grad():\n","            history_lossTe[epoch+1]=[]\n","            for k in range(num,num+len(mbb[num:])):\n","                Ex=model(mbb[k],btot)\n","                anchors=Ex.T\n","                currentB=k\n","                positives,negatives=model.getHardP_N(currentB,anchors)\n","\n","                lossTe=loss(anchors,positives.detach().to(device),negatives.detach().to(device))\n","                history_lossTe[epoch+1]=lossTe.item()\n","                \n","\n","            history_lossTr[epoch+1]=sum(history_lossTr[epoch+1])/len(history_lossTr[epoch+1])\n","\n","            print(\"Evaluating the epoch n°\",epoch+1)\n","            #accTr=model.evalAcc(training,KNN,bt)\n","            accTe=model.evalAcc(testing,KNN,btot)\n","            #TrainAcc=np.mean(np.array(accTr))\n","            TestAcc=np.mean(np.array(accTe))\n","            history_accuracy[epoch+1]=TestAcc\n","            print(\"Processesed epoch n° {},\\tTest accuracy: {:.4f}\\tTest Loss: {:.4f}\\tTrain Loss: {:.8f}\\t\".format((epoch+1),TestAcc,history_lossTe[epoch+1],history_lossTr[epoch+1]))\n","            endEP=time.time()\n","            scheduler.step()\n","            print(\"Requested time for processing {}-th epoch was: {:.4f} secs.\".format(epoch+1,endEP-startEP))\n","    return model, optimizer, history_lossTr, history_accuracy, history_lossTe\n","\n","def Save_Model(path,model, optimizer, history_lossTr, history_accuracy, history_lossTe):\n","    num_epochs=max(list(history_lossTr.keys()))\n","    checkpoint={\n","        \"epoch\":num_epochs,\n","        \"modelState\":model.state_dict(),\n","        \"optimizerState\":optimizer.state_dict(),\n","        \"Loss_trainHistory\":history_lossTr,\n","        \"Accuracy_History\":history_accuracy,\n","        \"Loss_testHistory\":history_lossTe\n","    }\n","    torch.save(checkpoint,path)\n","    \n","def Load_Model(path, device):\n","    model_checkpoint=torch.load(path, map_location = device)\n","    return model_checkpoint\n","\n","## For all the plots was used the plotly library, because it makes really easy to interact with the most famous data visualization tools. ##\n","def plot_arrays(metrics, conf_name, loss = True):\n","    num_epochs=len(metrics[0])\n","    fig1 = go.Figure()\n","    if loss:\n","      text = ['Loss on the TrainSet', 'Loss on the TestSet','TrainSet Loss VS. TestSet Loss in '+ conf_name,\"Loss\"]\n","\n","    \n","\n","      \n","\n","      fig1.add_trace(go.Scatter(x=np.array(range(num_epochs)), y=np.array(metrics[0]),\n","                  mode='lines+markers',\n","                  name=text[0], line=dict(color=\"navy\", width=4))) \n","    if loss == False:\n","      text = [\"acc\",\"Accuracy on TestSet\", \"Accuracy on TestSet in \"+ conf_name, \"Accuracy\"]\n","    \n","    fig1.add_trace(go.Scatter(x=np.array(range(num_epochs)), y= np.array(metrics[1]) if loss else np.array(metrics[0]),\n","                        mode='lines+markers',\n","                        name=text[1],line=dict(color=\"firebrick\", width=4)))\n","    \n","    fig1.update_layout(title=text[2],\n","                xaxis_title='epochs',\n","                yaxis_title=text[3])\n","    \n","\n","    fig1.show()\n","\n","def plot_metrics(train_diz,test_diz, conf_name, loss = True):\n","  if loss:\n","    metrics = [[train_diz[key] for key in list(sorted(train_diz.keys()))],[test_diz[key] for key in list(sorted(test_diz.keys()))]]\n","  else:\n","    metrics = [[test_diz[key] for key in list(sorted(test_diz.keys()))]]\n","  plot_arrays(metrics, conf_name, loss)\n","\n","def get_accuracy(model_name, device, instance, model_path = None, n_layers = False):\n","  models = [\"SAGE1\",\"SAGE2\", \"SAGE3\",\"conf1\",\"conf2\",\"conf3\",\"conf4\"]\n","  arch_paths = [\"models/one_layerSAGE.pt\", \"models/two_layerSAGE.pt\", \"models/three_layerSAGE.pt\", \"models/conf1.pt\", \"models/conf2.pt\", \"models/conf3.pt\", \"models/conf4.pt\"]\n","  btot = 2\n","  K = 200\n","  training=list(range(0,10189+1))\n","  testing=list(range(10190,11260+1))\n","  if model_name not in models:\n","    print(\"the model name inserted is not valid, choose one among these choices: \",models)\n","    return\n","  if n_layers != False:\n","    if model_path!= None:\n","      patt = model_path\n","    else:\n","      patt = arch_paths[n_layers-1]\n","    model = GraphSAGE(instance,A1,training,testing,n_layers,512, device, training_mode = False).to(torch.device(device))\n","    model_check = Load_Model(patt, device)\n","    model.load_state_dict(model_check['modelState'])\n","\n","    accuracy = model.evalAcc(testing,K,btot)\n","\n","    accuracy = np.mean(np.array(accuracy))\n","    \n","\n","    return accuracy\n","  \n","  else:\n","    if model_path!= None:\n","      patt = model_path\n","    else:\n","      patt = arch_paths[models.index(model_name)]\n","    if model_name == \"conf1\":\n","      model = Conf1(instance,A1,training,testing,512,device, training_mode = False).to(torch.device(device))\n","      model_check = Load_Model(patt, device)\n","      model.load_state_dict(model_check['modelState'])\n","      accuracy = model.evalAcc(testing,200,btot)\n","      accuracy = np.mean(np.array(accuracy))\n","      \n","      return accuracy\n","      \n","    elif model_name == \"conf2\":\n","      model = Conf2(instance,A1,training,testing,512,device, training_mode = False).to(torch.device(device))\n","      model_check = Load_Model(patt, device)\n","      model.load_state_dict(model_check['modelState'])\n","      accuracy = model.evalAcc(testing,K,btot)\n","\n","      accuracy = np.mean(np.array(accuracy))\n","    \n","\n","      return accuracy\n","\n","    elif model_name == \"conf3\":\n","      model = Conf3(instance,A1,training,testing,512,device, training_mode = False).to(torch.device(device))\n","      model_check = Load_Model(patt, device)\n","      model.load_state_dict(model_check['modelState'])\n","      accuracy = model.evalAcc(testing,K,btot)\n","\n","      accuracy = np.mean(np.array(accuracy))\n","      \n","\n","      return accuracy\n","\n","    elif model_name == \"conf4\":\n","\n","      model = Conf4(instance,A1,training,testing,512,device, training_mode = False).to(torch.device(device))\n","      model_check = Load_Model(patt, device)\n","      model.load_state_dict(model_check['modelState'])\n","      accuracy = model.evalAcc(testing,K,btot)\n","\n","      accuracy = np.mean(np.array(accuracy))\n","      \n","\n","      return accuracy\n","\n","def get_nearest_artists(embedding, artist_name, K, artist_to_id, id_to_artist):\n","  Knew = K+50\n","  T=embedding.detach().to(torch.device(\"cpu\")).numpy().transpose()\n","  neigh=NearestNeighbors(n_neighbors=Knew,algorithm='kd_tree').fit(T)#With the K-NN we get the nearest \n","  dist,ind = neigh.kneighbors(T[int(artist_to_id[artist_name])].reshape((1,-1))) \n","  \n","  artist_id = artist_to_id[artist_name]\n","\n","  neighbors_list = list(ind[0])[1:]\n","  neighbors_ = []\n","  c = 1\n","  while len(neighbors_)<K:\n","    if id_to_artist[str(neighbors_list[c])]!=None:\n","      neighbors_.append(id_to_artist[str(neighbors_list[c])])\n","      c+=1\n","    else:\n","      c+=1\n","\n","  #neighbors_list = [id_to_artist[str(artist)] for artist in neighbors_list if str(artist) in id_to_artist]\n","  \n","  return neighbors_\n","\n","def get_embeddings(model_name, device, model_path = None, n_layers = False):\n","  models = [\"SAGE1\",\"SAGE2\", \"SAGE3\",\"conf1\",\"conf2\",\"conf3\",\"conf4\"]\n","  arch_paths = [\"models/one_layerSAGE.pt\", \"models/two_layerSAGE.pt\", \"models/three_layerSAGE.pt\", \"models/conf1.pt\", \"models/conf2.pt\", \"models/conf3.pt\", \"models/conf4.pt\"]\n","  btot = 2\n","  training=list(range(0,10189+1))\n","  testing=list(range(10190,11260+1))\n","  if model_name not in models:\n","    print(\"the model name inserted is not valid, choose one among these choices: \",models)\n","    return\n","  if n_layers != False:\n","    if model_path!= None:\n","      patt = model_path\n","    else:\n","      patt = arch_paths[n_layers-1]\n","    model = GraphSAGE(X1,A1,training,testing,n_layers,512, device, training_mode = False).to(torch.device(device))\n","    model_check = Load_Model(patt, device)\n","    model.load_state_dict(model_check['modelState'])\n","    embeddings = model(training+testing, btot)\n","\n","    \n","\n","    return embeddings\n","  \n","  else:\n","    if model_path!= None:\n","      patt = model_path\n","    else:\n","      patt = arch_paths[models.index(model_name)]\n","    if model_name == \"conf1\":\n","      model = Conf1(X1,A1,training,testing,512,device, training_mode = False).to(torch.device(device))\n","      model_check = Load_Model(patt, device)\n","      model.load_state_dict(model_check['modelState'])\n","      embeddings = model(training+testing, btot)\n","      return embeddings\n","      \n","    elif model_name == \"conf2\":\n","      model = Conf2(X1,A1,training,testing,512,device, training_mode = False).to(torch.device(device))\n","      model_check = Load_Model(patt, device)\n","      model.load_state_dict(model_check['modelState'])\n","      embeddings = model(training+testing, btot)\n","      return embeddings\n","\n","    elif model_name == \"conf3\":\n","      model = Conf3(X1,A1,training,testing,512,device, training_mode = False).to(torch.device(device))\n","      model_check = Load_Model(patt, device)\n","      model.load_state_dict(model_check['modelState'])\n","      embeddings = model(training+testing, btot)\n","      return embeddings\n","\n","    elif model_name == \"conf4\":\n","      model = Conf4(X1,A1,training,testing,512,device, training_mode = False).to(torch.device(device))\n","      model_check = Load_Model(patt, device)\n","      model.load_state_dict(model_check['modelState'])\n","      embeddings = model(training+testing, btot)\n","      return embeddings\n","\n"],"id":"5e580b5e"},{"cell_type":"markdown","metadata":{"id":"4789afc0"},"source":["## First Configuration #1\n","\n","- Through torch_geometric we have to use the Coordinate format (COO), to represent the graph data structure.\n","- Thus, in the class definition for the first configuration requested we have to convert our Adjacency matrix in a COO format.\n","\n","- Network design requested:\n","\n","1. Linear(Input, 256) \n","2. Linear(256, 256)\n","3. GCNConv(256, 256)\n","4. GCNConv(256, 256)\n","5. TripletLoss()\n","\n","* GCNConv is an architecture that was presented in the paper ['Semi-Supervised Classification with Graph Convolutional Networks'](https://arxiv.org/abs/1609.02907), that is a way to encode in a latent representation the Graph's nodes, as in the case of GraphSAGE.  \n","In the end of the notebook the performances over all the architectures are compared."],"id":"4789afc0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f779093d"},"outputs":[],"source":["## All the methods for this class are the same as in the GraphSAGE class, the only changes are about the forward method and the constructor. ##  \n","\n","class Conf1(nn.Module): ## lr=0.0001 weigth_decay=0.01\n","    \n","    def __init__(self,X,A,train_set,test_set,batch_size,device, training_mode = True):\n","        super(Conf1,self).__init__()\n","        self.device=device\n","        self.A=A.to(self.device) #Tensors version of adjacency matrix and Instances\n","        self.Atrain=self.select(self.A,train_set,train_set).to(self.device)\n","        self.ATot=self.select(self.A,train_set+test_set,train_set+test_set).to(self.device)\n","        self.L=2\n","        self.X=torch.tensor(X.tolist(),requires_grad=True).to(self.device)   \n","        self.COO=torch.load('COOA.pt')\n","        self.train=train_set\n","        self.test=test_set\n","        self.lamb=0.8\n","        if training_mode:\n","          self.diz=self.getDiz()\n","          self.bs=batch_size\n","          self.mb=self.mini_batches(self.train+self.test,self.bs)\n","          self.OrDiz=self.getCorrenspondancies(self.mb)\n","        self.out=256\n","        self.l1=nn.Linear(2613,self.out)\n","        self.l2=nn.Linear(self.out,self.out)\n","        self.GCN1=GCNConv(self.out,self.out)\n","        self.GCN2=GCNConv(self.out,self.out)\n","        \n","        self.COO=torch.load('COOA.pt')\n","        \n","    \n","    def forward(self,V,b,nbs=-1,eval_mode = False):\n","        Vdiz=self.tracing(V,b, eval_mode)\n","        \n","        if len(V)>500 and nbs!=-1:\n","            OrDiz=self.OrDiz[nbs]\n","        else:\n","            OrDiz=self.getCorr(Vdiz)\n","        \n","        ## Fully Connected layers ##\n","        Es=self.select(self.X,set(),Vdiz[1]).T\n","        Es=F.elu(self.l1(Es))\n","        Es=F.elu(self.l2(Es))\n","        \n","        ## First Layer ##\n","        Anew=self.select(self.A,Vdiz[1],Vdiz[1])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=self.GCN1(Es,Anew).T\n","        Es=F.elu(Es)\n","        \n","        ## Second Layer ##\n","        Es=self.select(Es,set(),OrDiz[2].keys()).T\n","        Anew=self.select(self.A,Vdiz[2],Vdiz[2])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=self.GCN2(Es,Anew).T\n","        Es=F.elu(Es)\n","        Es=self.select(Es,set(),OrDiz[3])\n","        \n","        return Es\n","    \n","    def getCorrenspondancies(self,mbb):\n","        OrDiz={}\n","        num=int(len(self.train)/self.bs)+1\n","        for j in range(len(mbb[:num])):\n","          Vdiz=self.tracing(mbb[j],1)\n","          OrDiz[j]={}\n","          for k in Vdiz:\n","              OrDiz[j][k]={}\n","              OrDiz[j][k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","          print(\"Processed {}-th mini-batch\".format(j+1))\n","        return OrDiz\n","    \n","    def getCorr(self,Vdiz):\n","        OrDiz={}\n","        for k in Vdiz:\n","            OrDiz[k]={}\n","            OrDiz[k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","        return OrDiz\n","            \n","            \n","    \n","    def tracing(self,V,b,eval_mode = False):\n","        Vdiz={}\n","        K=self.L+1\n","        Vdiz[K]=sorted(list(V))\n","        for k in range(K-1,0,-1):\n","            d=set()\n","            for idx in Vdiz[k+1]: \n","                d=d.union(self.get_n(idx,b, eval_mode))\n","                \n","            Vdiz[k]=d\n","        return Vdiz\n","            \n","    def get_n(self,idx,b, eval_mode): #This function is the neighbor's function. Given a batch index we get its neighborhood.\n","        if b==1:\n","            A=self.Atrain\n","        else:\n","            A=self.ATot\n","        t=torch.nonzero(A[idx])\n","        s=set()\n","        \n","        for k in t:\n","            if t.shape[0]!=0 and eval_mode == False:\n","              s.add(k.item())\n","            \n","            elif eval_mode == True:\n","              if idx in self.test:\n","                if k.item() not in self.test:\n","                  s.add(k.item())\n","              else:\n","                s.add(k.item())\n","            \n","            else:\n","              continue\n","        s.add(idx)     \n","        return s\n","    \n","    def select(self,mat,row,col):  #Given a set of indices for rows or column or both, we get the respective elements.\n","        col=sorted(list(col))      #This is applied when we get the t matrix.\n","       \n","        c=0\n","        if row==set():\n","            col=torch.tensor(col,dtype=torch.int32).to(self.device)\n","            ma=torch.index_select(mat,1,col)\n","            return ma\n","        else:\n","            row=torch.tensor(sorted(list(row))).to(self.device)\n","            col=torch.tensor(col).to(self.device)\n","            ma=torch.index_select(mat,0,row)\n","            ma=torch.index_select(ma,1,col)\n","            return ma\n","    \n","   \n","    def getDiz(self):\n","        diz={}\n","        for k in range(self.COO.shape[1]):\n","            cn=int(self.COO[0][k].item())\n","            # if cn>self.train[-1]:\n","            #     break\n","            \n","            if cn not in diz:\n","                diz[cn]=[[int(self.COO[1][k])]]\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn].append([r])\n","            elif cn in diz:\n","                diz[cn][0].append(int(self.COO[1][k]))\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn][1].append(r)\n","                \n","        return diz\n","    def ConvertAtoCOO(self,SA):\n","        Anew=torch.nonzero(SA).T.type(torch.LongTensor)\n","        return Anew.to(self.device)\n","\n","    def getHardP_N(self,currentB,tensor):\n","      samples=self.mbbPosNeg[currentB]\n","      positives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      negatives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      mbbList=sorted(list(self.mb[currentB]))\n","      for k in samples:\n","        pos=samples[k][0]\n","        neg=samples[k][1]\n","        distDizP={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in pos}\n","        distDizN={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in neg}\n","        summP=sum(list(distDizP.values()))\n","        summN=sum(list(distDizN.values()))\n","        if len(distDizP)!=0 and summP!=0:\n","          distDizP={kdist:distDizP[kdist]/summP for kdist in distDizP}\n","        else:\n","          distDizP[k]=0.5\n","        if len(distDizN)!=0 and summN!=0:\n","          distDizN={kdist:distDizN[kdist]/summN for kdist in distDizN}\n","        else:\n","          distDizN[k]=0.5\n","        keysP=list(distDizP.keys())\n","        keysN=list(distDizN.keys())\n","        \n","        for dist in keysP:\n","          if distDizP[dist]>=self.lamb and len(distDizP)!=1:\n","            distDizP.pop(dist)\n","        \n","        for dist in keysN:\n","          if distDizN[dist]<= 1-self.lamb and len(distDizN)!=1:\n","            distDizN.pop(dist)\n","        \n","        \n","        positives[mbbList.index(k)]=tensor[mbbList.index(max(distDizP,key=distDizP.get))]\n","        negatives[mbbList.index(k)]=tensor[mbbList.index(min(distDizN,key=distDizN.get))]\n","      \n","      return positives,negatives\n","        \n","        \n","    \n","    \n","    def mini_batches(self,indices,bs=32): #This function generates a list of minibatches, of size bs\n","        indexN=indices.copy()           #sets are unordered data structure, so there is no need to shuffle them. \n","        indicesTot=[indexN[:len(self.train)],indexN[self.test[0]:]]\n","        mbList=[]\n","        self.mbbPosNeg={} \n","        c=0   \n","        num=int(len(self.train)/self.bs)+1     #Lists of lists of mini_batches indices \n","        for indicesN in indicesTot:\n","          u=0\n","          if len(indicesN)==len(indexN[self.test[0]:]):\n","            mb=set(indicesN)\n","            mbList.append(mb)\n","            self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","            update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","            self.mbbPosNeg[c].update(update)\n","            return mbList\n","          while len(indicesN)!=0:\n","              mb=set()\n","                                  #Inner list, with the indices of a particular mini_batch\n","              while len(mb)<bs:\n","                  if len(indicesN)==0:\n","                      mbList.append(mb)\n","                      self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                      update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                      self.mbbPosNeg[c].update(update)\n","                      u=1\n","                      c+=1\n","                      break\n","\n","                  r=choice(indicesN)\n","                  sample=indicesN.pop(indicesN.index(r))\n","                  mb.add(sample)\n","              if u!=1:\n","                self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                self.mbbPosNeg[c].update(update)\n","                mbList.append(mb)\n","                c+=1\n","        return mbList          #obj.mini_batches(#,bs=128) #: train_,train,val,test, we get lists of list of batches from here\n","    \n","    \n","    def getNegSample(self,key,mb):\n","      l=[]\n","      for j in mb:\n","        if j!=key:\n","          if key not in self.diz:\n","            l.append(j)\n","            if len(l)==4:\n","              return l\n","          else:\n","            if j not in self.diz[key][0]:\n","              l.append(j)\n","              if len(l)==4:\n","                return l  \n","\n","\n","\n","    def calcG(self,ID):  #This method is used for the evaluation of accuracy, in particular it computes the denominator\n","        if ID>200:       # as described in the paper.\n","            ID=200\n","        c=1\n","        somm=0\n","        while c<=ID:\n","            somm+=1/(math.log2(1+c))\n","            c+=1\n","        return somm\n","\n","    def evalAcc(self,S,kneigh,b):  #This function is to compute accuracy for the test and train set. \n","        S = sorted(S)\n","        S1 = sorted(self.train + S)\n","        \n","        T = self.forward(S1,b, nbs = -1, eval_mode = True)\n","\n","        test_embs = self.select(T,set(),S).to(torch.device('cpu')).detach().T.numpy()\n","        T=T.detach().to(torch.device(\"cpu\")).numpy().transpose()\n","        \n","        neigh=NearestNeighbors(n_neighbors=(kneigh+1),algorithm='ball_tree').fit(test_embs) #With the K-NN we get the nearest \n","        \n","        dist,ind=neigh.kneighbors(test_embs)   \n","        \n","        acc=[]\n","        A_acc=self.select(self.A,S,S)\n","\n","        c=0\n","        for k in S:\n","            summ=0\n","            # ideal=len([i for i in range(self.test[0],self.test[0]+A_acc[k,:].shape[0]) if A_acc[k,i]!=0]) \n","            \n","            ideal = A_acc[ind[c][0]].sum().item()\n","            \n","            \n","            \n","            den=self.calcG(ideal)\n","            if den==0:\n","                c+=1\n","                continue  \n","            for j in range(len(ind[c][1:])):\n","                if A_acc[ind[c][0]][ind[c][1:][j]].item()!=0:\n","                    summ+= 1/(math.log2(1+(j+1)))\n","                    \n","                else:\n","                    continue\n","            c+=1    \n","            summ/=den\n","            acc.append(summ)\n","        return acc\n","    \n","    \n","        \n","    \n","    \n","    \n","    \n","    "],"id":"f779093d"},{"cell_type":"markdown","metadata":{"id":"4a474560"},"source":["## Second Configuration #2\n","\n","- Through torch_geometric we have to use the Coordinate format (COO), to represent the graph data structure.\n","- Thus, in the class definition for the first configuration requested we have to convert our Adjacency matrix in a COO format.\n","\n","- Network design requested:\n","\n","1. GCNConv(Input, 256)\n","2. GraphConv(256, 256)\n","3. GCNConv(256, 256)\n","4. GCNConv(256, 256)\n","5. Linear(256, 256) (**new**)\n","6. Linear(256, 256) (**new**)\n","7. TripletLoss()\n","\n","* This is the only architecture which implements 4 Graph Layers, the others reach at most 3 layers. Moreover in this architecture there is also one 'GraphConv' layer that is a kind of layer introduced in the paper ['Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks'](https://arxiv.org/abs/1810.02244).\n","* At the beginning we tried this architecture without including the FCC layers, but we have noticed that the performances increased with their introduction. \n","* Although this Network seems to be the most complex, it is not the best at performing the Artist Similarity task.\n","\n","* The 'GCNConv' layer is the same as in the first configuration. \n","\n","In the end of the notebook the performances over all the architectures are compared."],"id":"4a474560"},{"cell_type":"code","execution_count":null,"metadata":{"id":"143b8411"},"outputs":[],"source":["## All the methods for this class are the same as in the GraphSAGE class, the only changes are about the forward method and the constructor. ##  \n","\n","class Conf2(nn.Module): ## lr=0.00001 weigth_decay=0.01\n","    \n","    def __init__(self,X,A,train_set,test_set,batch_size,device, training_mode = True):\n","        super(Conf2,self).__init__()\n","        self.device=device\n","        self.A=A.to(self.device) #Tensors version of adjacency matrix and Instances\n","        self.Atrain=self.select(self.A,train_set,train_set).to(self.device)\n","        self.ATot=self.select(self.A,train_set+test_set,train_set+test_set).to(self.device)\n","        self.L=4\n","        self.X=torch.tensor(X.tolist(),requires_grad=True).to(self.device)   \n","        self.COO=torch.load('COOA.pt')\n","        self.train=train_set\n","        self.test=test_set\n","        self.lamb=0.8\n","        if training_mode:\n","          self.diz=self.getDiz()\n","          self.bs=batch_size\n","          self.mb=self.mini_batches(self.train+self.test,self.bs)\n","          self.OrDiz=self.getCorrenspondancies(self.mb)\n","        self.n_input=2613\n","        \n","        self.out=256\n","        self.GCN1=GCNConv(self.n_input,self.out)\n","        self.Graph=GraphConv(self.out,self.out)\n","        self.GCN2=GCNConv(self.out,self.out)\n","        self.GCN3=GCNConv(self.out,self.out)\n","        \n","        self.l1 = nn.Linear(self.out,self.out)\n","        self.l2 = nn.Linear(self.out,self.out)\n","    \n","    def forward(self,V,b,nbs=-1,eval_mode = False):\n","        Vdiz=self.tracing(V,b, eval_mode)\n","        \n","        if len(V)>500 and nbs!=-1:\n","            OrDiz=self.OrDiz[nbs]\n","        else:\n","            OrDiz=self.getCorr(Vdiz)\n","        \n","        ### First Layer ####\n","        Es=self.select(self.X,set(),Vdiz[1]).T\n","        Anew=self.select(self.A,Vdiz[1],Vdiz[1])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=F.elu(self.GCN1(Es,Anew).T)\n","        \n","        ### Second Layer ###\n","        Es=self.select(Es,set(),OrDiz[2].keys()).T\n","        Anew=self.select(self.A,Vdiz[2],Vdiz[2])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=F.elu(self.Graph(Es,Anew).T)\n","        \n","        ### Third Layer ###\n","        Es=self.select(Es,set(),OrDiz[3].keys()).T\n","        Anew=self.select(self.A,Vdiz[3],Vdiz[3])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=F.elu(self.GCN2(Es,Anew).T)\n","        \n","        ### Fourth Layer ###\n","        Es=self.select(Es,set(),OrDiz[4].keys()).T\n","        Anew=self.select(self.A,Vdiz[4],Vdiz[4])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=F.elu(self.GCN3(Es,Anew).T)\n","        \n","        Es=self.select(Es,set(),OrDiz[5].keys())\n","        \n","        Es=F.elu(self.l1(Es.T))\n","        Es=F.elu(self.l2(Es))\n","\n","\n","        return Es.T\n","        \n","          \n","        \n","               \n","          \n","        \n","            \n","            \n","    def getCorrenspondancies(self,mbb):\n","        OrDiz={}\n","        num=int(len(self.train)/self.bs)+1\n","        for j in range(len(mbb[:num])):\n","          Vdiz=self.tracing(mbb[j],1)\n","          OrDiz[j]={}\n","          for k in Vdiz:\n","              OrDiz[j][k]={}\n","              OrDiz[j][k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","          print(\"Processed {}-th mini-batch\".format(j+1))\n","        return OrDiz\n","    \n","    def getCorr(self,Vdiz):\n","        OrDiz={}\n","        for k in Vdiz:\n","            OrDiz[k]={}\n","            OrDiz[k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","        return OrDiz\n","            \n","            \n","    \n","    def tracing(self,V,b,eval_mode = False):\n","        Vdiz={}\n","        K=self.L+1\n","        Vdiz[K]=sorted(list(V))\n","        for k in range(K-1,0,-1):\n","            d=set()\n","            for idx in Vdiz[k+1]: \n","                d=d.union(self.get_n(idx,b, eval_mode))\n","                \n","            Vdiz[k]=d\n","        return Vdiz\n","            \n","    def get_n(self,idx,b, eval_mode): #This function is the neighbor's function. Given a batch index we get its neighborhood.\n","        if b==1:\n","            A=self.Atrain\n","        else:\n","            A=self.ATot\n","        t=torch.nonzero(A[idx])\n","        s=set()\n","        \n","        for k in t:\n","            if t.shape[0]!=0 and eval_mode == False:\n","              s.add(k.item())\n","            \n","            elif eval_mode == True:\n","              if idx in self.test:\n","                if k.item() not in self.test:\n","                  s.add(k.item())\n","              else:\n","                s.add(k.item())\n","            \n","            else:\n","              continue\n","        s.add(idx)     \n","        return s\n","    \n","    def select(self,mat,row,col):  #Given a set of indices for rows or column or both, we get the respective elements.\n","        col=sorted(list(col))      #This is applied when we get the t matrix.\n","       \n","        c=0\n","        if row==set():\n","            col=torch.tensor(col,dtype=torch.int32).to(self.device)\n","            ma=torch.index_select(mat,1,col)\n","            return ma\n","        else:\n","            row=torch.tensor(sorted(list(row))).to(self.device)\n","            col=torch.tensor(col).to(self.device)\n","            ma=torch.index_select(mat,0,row)\n","            ma=torch.index_select(ma,1,col)\n","            return ma\n","    \n","   \n","    def getDiz(self):\n","        diz={}\n","        for k in range(self.COO.shape[1]):\n","            cn=int(self.COO[0][k].item())\n","            # if cn>self.train[-1]:\n","            #     break\n","            \n","            if cn not in diz:\n","                diz[cn]=[[int(self.COO[1][k])]]\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn].append([r])\n","            elif cn in diz:\n","                diz[cn][0].append(int(self.COO[1][k]))\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn][1].append(r)\n","                \n","        return diz\n","    def ConvertAtoCOO(self,SA):\n","        Anew=torch.nonzero(SA).T.type(torch.LongTensor)\n","        return Anew.to(self.device)\n","\n","    def getHardP_N(self,currentB,tensor):\n","      samples=self.mbbPosNeg[currentB]\n","      positives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      negatives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      mbbList=sorted(list(self.mb[currentB]))\n","      for k in samples:\n","        pos=samples[k][0]\n","        neg=samples[k][1]\n","        distDizP={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in pos}\n","        distDizN={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in neg}\n","        summP=sum(list(distDizP.values()))\n","        summN=sum(list(distDizN.values()))\n","        if len(distDizP)!=0 and summP!=0:\n","          distDizP={kdist:distDizP[kdist]/summP for kdist in distDizP}\n","        else:\n","          distDizP[k]=0.5\n","        if len(distDizN)!=0 and summN!=0:\n","          distDizN={kdist:distDizN[kdist]/summN for kdist in distDizN}\n","        else:\n","          distDizN[k]=0.5\n","        keysP=list(distDizP.keys())\n","        keysN=list(distDizN.keys())\n","        \n","        for dist in keysP:\n","          if distDizP[dist]>=self.lamb and len(distDizP)!=1:\n","            distDizP.pop(dist)\n","        \n","        for dist in keysN:\n","          if distDizN[dist]<= 1-self.lamb and len(distDizN)!=1:\n","            distDizN.pop(dist)\n","        \n","        \n","        positives[mbbList.index(k)]=tensor[mbbList.index(max(distDizP,key=distDizP.get))]\n","        negatives[mbbList.index(k)]=tensor[mbbList.index(min(distDizN,key=distDizN.get))]\n","      \n","      return positives,negatives\n","        \n","        \n","    \n","    \n","    def mini_batches(self,indices,bs=32): #This function generates a list of minibatches, of size bs\n","        indexN=indices.copy()           #sets are unordered data structure, so there is no need to shuffle them. \n","        indicesTot=[indexN[:len(self.train)],indexN[self.test[0]:]]\n","        mbList=[]\n","        self.mbbPosNeg={} \n","        c=0   \n","        num=int(len(self.train)/self.bs)+1     #Lists of lists of mini_batches indices \n","        for indicesN in indicesTot:\n","          u=0\n","          if len(indicesN)==len(indexN[self.test[0]:]):\n","            mb=set(indicesN)\n","            mbList.append(mb)\n","            self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","            update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","            self.mbbPosNeg[c].update(update)\n","            return mbList\n","          while len(indicesN)!=0:\n","              mb=set()\n","                                  #Inner list, with the indices of a particular mini_batch\n","              while len(mb)<bs:\n","                  if len(indicesN)==0:\n","                      mbList.append(mb)\n","                      self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                      update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                      self.mbbPosNeg[c].update(update)\n","                      u=1\n","                      c+=1\n","                      break\n","\n","                  r=choice(indicesN)\n","                  sample=indicesN.pop(indicesN.index(r))\n","                  mb.add(sample)\n","              if u!=1:\n","                self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                self.mbbPosNeg[c].update(update)\n","                mbList.append(mb)\n","                c+=1\n","        return mbList          #obj.mini_batches(#,bs=128) #: train_,train,val,test, we get lists of list of batches from here\n","    \n","    \n","    def getNegSample(self,key,mb):\n","      l=[]\n","      for j in mb:\n","        if j!=key:\n","          if key not in self.diz:\n","            l.append(j)\n","            if len(l)==4:\n","              return l\n","          else:\n","            if j not in self.diz[key][0]:\n","              l.append(j)\n","              if len(l)==4:\n","                return l  \n","\n","\n","\n","    def calcG(self,ID):  #This method is used for the evaluation of accuracy, in particular it computes the denominator\n","        if ID>200:       # as described in the paper.\n","            ID=200\n","        c=1\n","        somm=0\n","        while c<=ID:\n","            somm+=1/(math.log2(1+c))\n","            c+=1\n","        return somm\n","\n","    def evalAcc(self,S,kneigh,b):  #This function is to compute accuracy for the test and train set. \n","        S = sorted(S)\n","        S1 = sorted(self.train + S)\n","        \n","        T = self.forward(S1,b, nbs = -1, eval_mode = True)\n","\n","        test_embs = self.select(T,set(),S).to(torch.device('cpu')).detach().T.numpy()\n","        T=T.detach().to(torch.device(\"cpu\")).numpy().transpose()\n","        \n","        neigh=NearestNeighbors(n_neighbors=(kneigh+1),algorithm='ball_tree').fit(test_embs) #With the K-NN we get the nearest \n","        \n","        dist,ind=neigh.kneighbors(test_embs)   \n","        \n","        acc=[]\n","        A_acc=self.select(self.A,S,S)\n","\n","        c=0\n","        for k in S:\n","            summ=0\n","            # ideal=len([i for i in range(self.test[0],self.test[0]+A_acc[k,:].shape[0]) if A_acc[k,i]!=0]) \n","            \n","            ideal = A_acc[ind[c][0]].sum().item()\n","            \n","            \n","            \n","            den=self.calcG(ideal)\n","            if den==0:\n","                c+=1\n","                continue  \n","            for j in range(len(ind[c][1:])):\n","                if A_acc[ind[c][0]][ind[c][1:][j]].item()!=0:\n","                    summ+= 1/(math.log2(1+(j+1)))\n","                    \n","                else:\n","                    continue\n","            c+=1    \n","            summ/=den\n","            acc.append(summ)\n","        return acc\n","        \n","        \n","    \n","    \n","    \n","    \n"],"id":"143b8411"},{"cell_type":"markdown","metadata":{"id":"dc8899a8"},"source":["## Third Configuration #3\n","\n","- Through torch_geometric we have to use the Coordinate format (COO), to represent the graph data structure.\n","- Thus, in the class definition for the first configuration requested we have to convert our Adjacency matrix in a COO format.\n","\n","- Network design requested:\n","\n","1. Linear(Input, 256)\n","2. Linear(256, 256)\n","3. Linear(256, 256) (**new**)\n","3. GATConv(256, 256)\n","4. GATConv(256, 256)\n","5. TripletLoss()\n","\n","* This configuration appears to be way better than the others, it is not easy to explain why this happens, but is amazing to see how it outperforms the other architectures, even though it has just 2 Graph layers.\n","* Also in this case there were made some modifications in the architecture, indeed at the beginning there were just 2 FCC layers, but by introducing another layer we have slightly improved our perfomances on the task.\n","* In this architecture the Graph layers are the ones described in [Graph Attention Networks](https://arxiv.org/abs/1710.10903).\n","\n","In the end of the notebook the performances over all the architectures are compared."],"id":"dc8899a8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3c6191ba"},"outputs":[],"source":["## All the methods for this class are the same as in the GraphSAGE class, the only changes are about the forward method and the constructor. ##  \n","\n","class Conf3(nn.Module):   ## lr=0.0001 weigth_decay=0.01\n","    \n","    def __init__(self,X,A,train_set,test_set,batch_size,device, training_mode = True):\n","        super(Conf3,self).__init__()\n","        self.device=device\n","        self.A=A.to(self.device) #Tensors version of adjacency matrix and Instances\n","        self.Atrain=self.select(self.A,train_set,train_set).to(self.device)\n","        self.ATot=self.select(self.A,train_set+test_set,train_set+test_set).to(self.device)\n","        self.L=2\n","        self.X=torch.tensor(X.tolist(),requires_grad=True).to(self.device)   \n","        self.COO=torch.load('COOA.pt')\n","        self.train=train_set\n","        self.test=test_set\n","        self.lamb=0.8\n","        if training_mode:\n","          self.diz=self.getDiz()\n","          self.bs=batch_size\n","          self.mb=self.mini_batches(self.train+self.test,self.bs)\n","          self.OrDiz=self.getCorrenspondancies(self.mb)\n","          \n","        self.n_input=2613\n","        self.n_out=256\n","        self.l1=nn.Linear(self.n_input,self.n_out)\n","        self.l2=nn.Linear(self.n_out,self.n_out)\n","        self.l3=nn.Linear(self.n_out,self.n_out)\n","\n","        self.GAT1=GATConv(self.n_out,self.n_out)\n","        self.GAT2=GATConv(self.n_out,self.n_out)\n","        \n","    \n","    \n","    def forward(self,V,b,nbs=-1,eval_mode = False):\n","        Vdiz=self.tracing(V,b, eval_mode)\n","        \n","        if len(V)>500 and nbs!=-1:\n","            OrDiz=self.OrDiz[nbs]\n","        else:\n","            OrDiz=self.getCorr(Vdiz)\n","        \n","        ## Fully Connected layers ##\n","        Es=self.select(self.X,set(),Vdiz[1]).T\n","        Es=F.elu(self.l1(Es))\n","        Es=F.elu(self.l2(Es))\n","        Es=F.elu(self.l3(Es))\n","\n","        \n","        ## First Layer ##\n","        Anew=self.select(self.A,Vdiz[1],Vdiz[1])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=self.GAT1(Es,Anew).T\n","        Es=F.elu(Es)\n","        \n","        ## Second Layer ##\n","        Es=self.select(Es,set(),OrDiz[2].keys()).T\n","        Anew=self.select(self.A,Vdiz[2],Vdiz[2])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=self.GAT2(Es,Anew).T\n","        Es=F.elu(Es)\n","        Es=self.select(Es,set(),OrDiz[3])\n","        \n","        return Es\n","    \n","    def getCorrenspondancies(self,mbb):\n","        OrDiz={}\n","        num=int(len(self.train)/self.bs)+1\n","        for j in range(len(mbb[:num])):\n","          Vdiz=self.tracing(mbb[j],1)\n","          OrDiz[j]={}\n","          for k in Vdiz:\n","              OrDiz[j][k]={}\n","              OrDiz[j][k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","          print(\"Processed {}-th mini-batch\".format(j+1))\n","        return OrDiz\n","    \n","    def getCorr(self,Vdiz):\n","        OrDiz={}\n","        for k in Vdiz:\n","            OrDiz[k]={}\n","            OrDiz[k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","        return OrDiz\n","            \n","            \n","    \n","    def tracing(self,V,b,eval_mode = False):\n","        Vdiz={}\n","        K=self.L+1\n","        Vdiz[K]=sorted(list(V))\n","        for k in range(K-1,0,-1):\n","            d=set()\n","            for idx in Vdiz[k+1]: \n","                d=d.union(self.get_n(idx,b, eval_mode))\n","                \n","            Vdiz[k]=d\n","        return Vdiz\n","            \n","    def get_n(self,idx,b, eval_mode): #This function is the neighbor's function. Given a batch index we get its neighborhood.\n","        if b==1:\n","            A=self.Atrain\n","        else:\n","            A=self.ATot\n","        t=torch.nonzero(A[idx])\n","        s=set()\n","        \n","        for k in t:\n","            if t.shape[0]!=0 and eval_mode == False:\n","              s.add(k.item())\n","            \n","            elif eval_mode == True:\n","              if idx in self.test:\n","                if k.item() not in self.test:\n","                  s.add(k.item())\n","              else:\n","                s.add(k.item())\n","            \n","            else:\n","              continue\n","        s.add(idx)     \n","        return s\n","    \n","    def select(self,mat,row,col):  #Given a set of indices for rows or column or both, we get the respective elements.\n","        col=sorted(list(col))      #This is applied when we get the t matrix.\n","       \n","        c=0\n","        if row==set():\n","            col=torch.tensor(col,dtype=torch.int32).to(self.device)\n","            ma=torch.index_select(mat,1,col)\n","            return ma\n","        else:\n","            row=torch.tensor(sorted(list(row))).to(self.device)\n","            col=torch.tensor(col).to(self.device)\n","            ma=torch.index_select(mat,0,row)\n","            ma=torch.index_select(ma,1,col)\n","            return ma\n","    \n","   \n","    def getDiz(self):\n","        diz={}\n","        for k in range(self.COO.shape[1]):\n","            cn=int(self.COO[0][k].item())\n","            # if cn>self.train[-1]:\n","            #     break\n","            \n","            if cn not in diz:\n","                diz[cn]=[[int(self.COO[1][k])]]\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn].append([r])\n","            elif cn in diz:\n","                diz[cn][0].append(int(self.COO[1][k]))\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn][1].append(r)\n","                \n","        return diz\n","    def ConvertAtoCOO(self,SA):\n","        Anew=torch.nonzero(SA).T.type(torch.LongTensor)\n","        return Anew.to(self.device)\n","\n","    def getHardP_N(self,currentB,tensor):\n","      samples=self.mbbPosNeg[currentB]\n","      positives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      negatives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      mbbList=sorted(list(self.mb[currentB]))\n","      for k in samples:\n","        pos=samples[k][0]\n","        neg=samples[k][1]\n","        distDizP={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in pos}\n","        distDizN={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in neg}\n","        summP=sum(list(distDizP.values()))\n","        summN=sum(list(distDizN.values()))\n","        if len(distDizP)!=0 and summP!=0:\n","          distDizP={kdist:distDizP[kdist]/summP for kdist in distDizP}\n","        else:\n","          distDizP[k]=0.5\n","        if len(distDizN)!=0 and summN!=0:\n","          distDizN={kdist:distDizN[kdist]/summN for kdist in distDizN}\n","        else:\n","          distDizN[k]=0.5\n","        keysP=list(distDizP.keys())\n","        keysN=list(distDizN.keys())\n","        \n","        for dist in keysP:\n","          if distDizP[dist]>=self.lamb and len(distDizP)!=1:\n","            distDizP.pop(dist)\n","        \n","        for dist in keysN:\n","          if distDizN[dist]<= 1-self.lamb and len(distDizN)!=1:\n","            distDizN.pop(dist)\n","        \n","        \n","        positives[mbbList.index(k)]=tensor[mbbList.index(max(distDizP,key=distDizP.get))]\n","        negatives[mbbList.index(k)]=tensor[mbbList.index(min(distDizN,key=distDizN.get))]\n","      \n","      return positives,negatives\n","        \n","        \n","    \n","    \n","    def mini_batches(self,indices,bs=32): #This function generates a list of minibatches, of size bs\n","        indexN=indices.copy()           #sets are unordered data structure, so there is no need to shuffle them. \n","        indicesTot=[indexN[:len(self.train)],indexN[self.test[0]:]]\n","        mbList=[]\n","        self.mbbPosNeg={} \n","        c=0   \n","        num=int(len(self.train)/self.bs)+1     #Lists of lists of mini_batches indices \n","        for indicesN in indicesTot:\n","          u=0\n","          if len(indicesN)==len(indexN[self.test[0]:]):\n","            mb=set(indicesN)\n","            mbList.append(mb)\n","            self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","            update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","            self.mbbPosNeg[c].update(update)\n","            return mbList\n","          while len(indicesN)!=0:\n","              mb=set()\n","                                  #Inner list, with the indices of a particular mini_batch\n","              while len(mb)<bs:\n","                  if len(indicesN)==0:\n","                      mbList.append(mb)\n","                      self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                      update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                      self.mbbPosNeg[c].update(update)\n","                      u=1\n","                      c+=1\n","                      break\n","\n","                  r=choice(indicesN)\n","                  sample=indicesN.pop(indicesN.index(r))\n","                  mb.add(sample)\n","              if u!=1:\n","                self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                self.mbbPosNeg[c].update(update)\n","                mbList.append(mb)\n","                c+=1\n","        return mbList          #obj.mini_batches(#,bs=128) #: train_,train,val,test, we get lists of list of batches from here\n","    \n","    \n","    def getNegSample(self,key,mb):\n","      l=[]\n","      for j in mb:\n","        if j!=key:\n","          if key not in self.diz:\n","            l.append(j)\n","            if len(l)==4:\n","              return l\n","          else:\n","            if j not in self.diz[key][0]:\n","              l.append(j)\n","              if len(l)==4:\n","                return l  \n","\n","\n","\n","    def calcG(self,ID):  #This method is used for the evaluation of accuracy, in particular it computes the denominator\n","        if ID>200:       # as described in the paper.\n","            ID=200\n","        c=1\n","        somm=0\n","        while c<=ID:\n","            somm+=1/(math.log2(1+c))\n","            c+=1\n","        return somm\n","\n","    def evalAcc(self,S,kneigh,b):  #This function is to compute accuracy for the test and train set. \n","        S = sorted(S)\n","        S1 = sorted(self.train + S)\n","        \n","        T = self.forward(S1,b, nbs = -1, eval_mode = True)\n","\n","        test_embs = self.select(T,set(),S).to(torch.device('cpu')).detach().T.numpy()\n","        T=T.detach().to(torch.device(\"cpu\")).numpy().transpose()\n","        \n","        neigh=NearestNeighbors(n_neighbors=(kneigh+1),algorithm='ball_tree').fit(test_embs) #With the K-NN we get the nearest \n","        \n","        dist,ind=neigh.kneighbors(test_embs)   \n","        \n","        acc=[]\n","        A_acc=self.select(self.A,S,S)\n","\n","        c=0\n","        for k in S:\n","            summ=0\n","            # ideal=len([i for i in range(self.test[0],self.test[0]+A_acc[k,:].shape[0]) if A_acc[k,i]!=0]) \n","            \n","            ideal = A_acc[ind[c][0]].sum().item()\n","            \n","            \n","            \n","            den=self.calcG(ideal)\n","            if den==0:\n","                c+=1\n","                continue  \n","            for j in range(len(ind[c][1:])):\n","                if A_acc[ind[c][0]][ind[c][1:][j]].item()!=0:\n","                    summ+= 1/(math.log2(1+(j+1)))\n","                    \n","                else:\n","                    continue\n","            c+=1    \n","            summ/=den\n","            acc.append(summ)\n","        return acc\n","            \n","      \n","    "],"id":"3c6191ba"},{"cell_type":"markdown","metadata":{"id":"a31428c7"},"source":["## Fourth Configuration ##\n","\n","- Through torch_geometric we have to use the Coordinate format (COO), to represent the graph data structure.\n","- Thus, in the class definition for the first configuration requested we have to convert our Adjacency matrix in a COO format.\n","\n","- Network design requested:\n","\n","1. GATConv(Input, 256)\n","2. GATConv(256, 256)\n","3. Linear(256, 256)\n","4. Linear(256, 256)\n","5. TripletLoss()\n","\n","* Also with this 'GAT-based' architecture we obtain good results, indeed this is similar to the third configuration, but the FCC layers are localized right after the Graph layers, similarly to the GraphSAGE's architectures.\n","\n","In the end of the notebook the performances over all the architectures are compared."],"id":"a31428c7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bd8e1414"},"outputs":[],"source":["## All the methods for this class are the same as in the GraphSAGE class, the only changes are about the forward method and the constructor. ##  \n","\n","class Conf4(nn.Module):  ## lr=0.00001 weigth_decay=0.01\n","    \n","    def __init__(self,X,A,train_set,test_set,batch_size,device, training_mode = True):\n","        super(Conf4,self).__init__()\n","        self.device=device\n","        self.A=A.to(self.device) #Tensors version of adjacency matrix and Instances\n","        self.Atrain=self.select(self.A,train_set,train_set).to(self.device)\n","        self.ATot=self.select(self.A,train_set+test_set,train_set+test_set).to(self.device)\n","        self.L=2\n","        self.X=torch.tensor(X.tolist(),requires_grad=True).to(self.device)   \n","        self.COO=torch.load('COOA.pt')\n","        self.train=train_set\n","        self.test=test_set\n","        self.lamb=0.8\n","        if training_mode:\n","          self.diz=self.getDiz()\n","          self.bs=batch_size\n","          self.mb=self.mini_batches(self.train+self.test,self.bs)\n","          self.OrDiz=self.getCorrenspondancies(self.mb)\n","        \n","        self.n_input=2613\n","        self.n_out=256\n","        self.l1=nn.Linear(self.n_out,self.n_out)\n","        self.l2=nn.Linear(self.n_out,self.n_out)\n","\n","        self.GAT1=GATConv(self.n_input,self.n_out)\n","        self.GAT2=GATConv(self.n_out,self.n_out)\n","        \n","    \n","    \n","    def forward(self,V,b,nbs=-1,eval_mode = False):\n","        Vdiz=self.tracing(V,b, eval_mode)\n","        \n","        if len(V)>500 and nbs!=-1:\n","            OrDiz=self.OrDiz[nbs]\n","        else:\n","            OrDiz=self.getCorr(Vdiz)\n","        \n","        ## First Layer ##\n","        Es=self.select(self.X,set(),Vdiz[1]).T\n","        Anew=self.select(self.A,Vdiz[1],Vdiz[1])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=self.GAT1(Es,Anew).T\n","        Es=F.elu(Es)\n","        ## Second Layer ##\n","        Es=self.select(Es,set(),OrDiz[2].keys()).T\n","        Anew=self.select(self.A,Vdiz[2],Vdiz[2])\n","        Anew=self.ConvertAtoCOO(Anew)\n","        Es=self.GAT2(Es,Anew)\n","        Es=F.elu(Es).T\n","        \n","        Es=self.select(Es,set(),OrDiz[3]).T\n","        ## Fully Connected layers ##\n","        \n","        Es=F.elu(self.l1(Es))\n","        \n","        Es=F.elu(self.l2(Es))\n","\n","        \n","        \n","        \n","        return Es.T\n","    \n","    def getCorrenspondancies(self,mbb):\n","        OrDiz={}\n","        num=int(len(self.train)/self.bs)+1\n","        for j in range(len(mbb[:num])):\n","          Vdiz=self.tracing(mbb[j],1)\n","          OrDiz[j]={}\n","          for k in Vdiz:\n","              OrDiz[j][k]={}\n","              OrDiz[j][k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","          print(\"Processed {}-th mini-batch\".format(j+1))\n","        return OrDiz\n","    \n","    def getCorr(self,Vdiz):\n","        OrDiz={}\n","        for k in Vdiz:\n","            OrDiz[k]={}\n","            OrDiz[k]={i: sorted(list(Vdiz[k]))[i] for i in range(len(Vdiz[k]))}\n","        return OrDiz\n","            \n","            \n","    \n","    def tracing(self,V,b,eval_mode = False):\n","        Vdiz={}\n","        K=self.L+1\n","        Vdiz[K]=sorted(list(V))\n","        for k in range(K-1,0,-1):\n","            d=set()\n","            for idx in Vdiz[k+1]: \n","                d=d.union(self.get_n(idx,b, eval_mode))\n","                \n","            Vdiz[k]=d\n","        return Vdiz\n","            \n","    def get_n(self,idx,b, eval_mode): #This function is the neighbor's function. Given a batch index we get its neighborhood.\n","        if b==1:\n","            A=self.Atrain\n","        else:\n","            A=self.ATot\n","        t=torch.nonzero(A[idx])\n","        s=set()\n","        \n","        for k in t:\n","            if t.shape[0]!=0 and eval_mode == False:\n","              s.add(k.item())\n","            \n","            elif eval_mode == True:\n","              if idx in self.test:\n","                if k.item() not in self.test:\n","                  s.add(k.item())\n","              else:\n","                s.add(k.item())\n","            \n","            else:\n","              continue\n","        s.add(idx)     \n","        return s\n","    \n","    def select(self,mat,row,col):  #Given a set of indices for rows or column or both, we get the respective elements.\n","        col=sorted(list(col))      #This is applied when we get the t matrix.\n","       \n","        c=0\n","        if row==set():\n","            col=torch.tensor(col,dtype=torch.int32).to(self.device)\n","            ma=torch.index_select(mat,1,col)\n","            return ma\n","        else:\n","            row=torch.tensor(sorted(list(row))).to(self.device)\n","            col=torch.tensor(col).to(self.device)\n","            ma=torch.index_select(mat,0,row)\n","            ma=torch.index_select(ma,1,col)\n","            return ma\n","    \n","   \n","    def getDiz(self):\n","        diz={}\n","        for k in range(self.COO.shape[1]):\n","            cn=int(self.COO[0][k].item())\n","            # if cn>self.train[-1]:\n","            #     break\n","            \n","            if cn not in diz:\n","                diz[cn]=[[int(self.COO[1][k])]]\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn].append([r])\n","            elif cn in diz:\n","                diz[cn][0].append(int(self.COO[1][k]))\n","                r=randrange(len(self.train))\n","                while self.A[int(cn)][r]!=0:\n","                    r=randrange(len(self.train))\n","                diz[cn][1].append(r)\n","                \n","        return diz\n","    def ConvertAtoCOO(self,SA):\n","        Anew=torch.nonzero(SA).T.type(torch.LongTensor)\n","        return Anew.to(self.device)\n","\n","    def getHardP_N(self,currentB,tensor):\n","      samples=self.mbbPosNeg[currentB]\n","      positives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      negatives=torch.zeros((tensor.shape[0],tensor.shape[1]))\n","      mbbList=sorted(list(self.mb[currentB]))\n","      for k in samples:\n","        pos=samples[k][0]\n","        neg=samples[k][1]\n","        distDizP={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in pos}\n","        distDizN={kdist:((tensor[mbbList.index(k)]-tensor[mbbList.index(kdist)])**2).sum().item() for kdist in neg}\n","        summP=sum(list(distDizP.values()))\n","        summN=sum(list(distDizN.values()))\n","        if len(distDizP)!=0 and summP!=0:\n","          distDizP={kdist:distDizP[kdist]/summP for kdist in distDizP}\n","        else:\n","          distDizP[k]=0.5\n","        if len(distDizN)!=0 and summN!=0:\n","          distDizN={kdist:distDizN[kdist]/summN for kdist in distDizN}\n","        else:\n","          distDizN[k]=0.5\n","        keysP=list(distDizP.keys())\n","        keysN=list(distDizN.keys())\n","        \n","        for dist in keysP:\n","          if distDizP[dist]>=self.lamb and len(distDizP)!=1:\n","            distDizP.pop(dist)\n","        \n","        for dist in keysN:\n","          if distDizN[dist]<= 1-self.lamb and len(distDizN)!=1:\n","            distDizN.pop(dist)\n","        \n","        \n","        positives[mbbList.index(k)]=tensor[mbbList.index(max(distDizP,key=distDizP.get))]\n","        negatives[mbbList.index(k)]=tensor[mbbList.index(min(distDizN,key=distDizN.get))]\n","      \n","      return positives,negatives\n","        \n","        \n","    \n","    \n","    def mini_batches(self,indices,bs=32): #This function generates a list of minibatches, of size bs\n","        indexN=indices.copy()           #sets are unordered data structure, so there is no need to shuffle them. \n","        indicesTot=[indexN[:len(self.train)],indexN[self.test[0]:]]\n","        mbList=[]\n","        self.mbbPosNeg={} \n","        c=0   \n","        num=int(len(self.train)/self.bs)+1     #Lists of lists of mini_batches indices \n","        for indicesN in indicesTot:\n","          u=0\n","          if len(indicesN)==len(indexN[self.test[0]:]):\n","            mb=set(indicesN)\n","            mbList.append(mb)\n","            self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","            update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","            self.mbbPosNeg[c].update(update)\n","            return mbList\n","          while len(indicesN)!=0:\n","              mb=set()\n","                                  #Inner list, with the indices of a particular mini_batch\n","              while len(mb)<bs:\n","                  if len(indicesN)==0:\n","                      mbList.append(mb)\n","                      self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                      update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                      self.mbbPosNeg[c].update(update)\n","                      u=1\n","                      c+=1\n","                      break\n","\n","                  r=choice(indicesN)\n","                  sample=indicesN.pop(indicesN.index(r))\n","                  mb.add(sample)\n","              if u!=1:\n","                self.mbbPosNeg[c]={key:[[i for i in set(self.diz[key][0]).intersection(mb)],self.getNegSample(key,mb)] for key in mb.intersection(set(self.diz.keys()))}\n","                update={key:[[key],self.getNegSample(key,mb)] for key in mb.difference(set(self.diz.keys()))}\n","                self.mbbPosNeg[c].update(update)\n","                mbList.append(mb)\n","                c+=1\n","        return mbList          #obj.mini_batches(#,bs=128) #: train_,train,val,test, we get lists of list of batches from here\n","    \n","    \n","    def getNegSample(self,key,mb):\n","      l=[]\n","      for j in mb:\n","        if j!=key:\n","          if key not in self.diz:\n","            l.append(j)\n","            if len(l)==4:\n","              return l\n","          else:\n","            if j not in self.diz[key][0]:\n","              l.append(j)\n","              if len(l)==4:\n","                return l  \n","\n","\n","\n","    def calcG(self,ID):  #This method is used for the evaluation of accuracy, in particular it computes the denominator\n","        if ID>200:       # as described in the paper.\n","            ID=200\n","        c=1\n","        somm=0\n","        while c<=ID:\n","            somm+=1/(math.log2(1+c))\n","            c+=1\n","        return somm\n","\n","    def evalAcc(self,S,kneigh,b):  #This function is to compute accuracy for the test and train set. \n","        S = sorted(S)\n","        S1 = sorted(self.train + S)\n","        \n","        T = self.forward(S1,b, nbs = -1, eval_mode = True)\n","\n","        test_embs = self.select(T,set(),S).to(torch.device('cpu')).detach().T.numpy()\n","        T=T.detach().to(torch.device(\"cpu\")).numpy().transpose()\n","        \n","        neigh=NearestNeighbors(n_neighbors=(kneigh+1),algorithm='ball_tree').fit(test_embs) #With the K-NN we get the nearest \n","        \n","        dist,ind=neigh.kneighbors(test_embs)   \n","        \n","        acc=[]\n","        A_acc=self.select(self.A,S,S)\n","\n","        c=0\n","        for k in S:\n","            summ=0\n","            # ideal=len([i for i in range(self.test[0],self.test[0]+A_acc[k,:].shape[0]) if A_acc[k,i]!=0]) \n","            \n","            ideal = A_acc[ind[c][0]].sum().item()\n","            \n","            \n","            \n","            den=self.calcG(ideal)\n","            if den==0:\n","                c+=1\n","                continue  \n","            for j in range(len(ind[c][1:])):\n","                if A_acc[ind[c][0]][ind[c][1:][j]].item()!=0:\n","                    summ+= 1/(math.log2(1+(j+1)))\n","                    \n","                else:\n","                    continue\n","            c+=1    \n","            summ/=den\n","            acc.append(summ)\n","        return acc\n","            \n","      \n","    "],"id":"bd8e1414"},{"cell_type":"markdown","metadata":{"id":"bb895db8"},"source":["# Training step\n","- In the following cell is possible to choose different hyperparameters to train the network.\n","- In the hyperparameters tuning we must take into account: the number of layer (they try from 0 to 3 graph layers), the batch size, and the dimension of the projection matrices (in the aggregation step).\n","- It is also possible to try previously described configurations.\n","- There are also other hyperparameters of course, but they already are  deeply described in the paper."],"id":"bb895db8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"06328f36"},"outputs":[],"source":["#### Possible partition of the dataset ####\n","train_=list(range(0,9021+1)) #Train set, without val\n","train=list(range(0,10189+1)) #Train set, with val\n","val=list(range(9022,10189+1))\n","test=list(range(10190,11260+1))"],"id":"06328f36"},{"cell_type":"code","execution_count":67,"metadata":{"id":"9ff785ed","scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652964494997,"user_tz":-120,"elapsed":159832,"user":{"displayName":"andrea giuseppe di francesco","userId":"08400322680190058161"}},"outputId":"82c9611c-2a9b-41c1-b75f-07da2dc00573"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processed 1-th mini-batch\n","Processed 2-th mini-batch\n","Processed 3-th mini-batch\n","Processed 4-th mini-batch\n","Processed 5-th mini-batch\n","Processed 6-th mini-batch\n","Processed 7-th mini-batch\n","Processed 8-th mini-batch\n","Processed 9-th mini-batch\n","Processed 10-th mini-batch\n","Processed 11-th mini-batch\n","Processed 12-th mini-batch\n","Processed 13-th mini-batch\n","Processed 14-th mini-batch\n","Processed 15-th mini-batch\n","Processed 16-th mini-batch\n","Processed 17-th mini-batch\n","Processed 18-th mini-batch\n","Processed 19-th mini-batch\n","Processed 20-th mini-batch\n","Adjusting learning rate of group 0 to 1.0000e-10.\n"]}],"source":["## train_/val stands for the splitting for the model selection/ validation, whereas the splitting train/test is for the comparison of the results.\n","training=train\n","testing=test\n","\n","n_layer=3      #n_of graph conv.layer, if we are using the GraphSAGE configuration.\n","batch_size=512 #This is the batch size used in the paper which inspired artist similarity, since it was not specified in the Artist similarity paper.\n","\n","device=torch.device(\"cuda\")\n","XX = torch.randn((2613,11261))\n","model= GraphSAGE(X1,A1,training,testing,n_layer,batch_size,device).to(device) #Conf3(X1,A1,training,testing,batch_size,device).to(device)\n","num_epochs=10 #According to the paper there will be 50 epochs for each experiment \n","\n","triplet_loss = nn.TripletMarginLoss(margin=0.2, p=2)\n","KNN=200\n","#### Tune the learning rate and the weight decay regularization term ####\n","lr=1e-10                         #[1e-6,1e-7,1e-8,1e-9,1-10]\n","weight_d=0.01\n","#lr=min([1,((1-0.9)/2)])    #linear warm-up described in the paper, beta2 = 0.9\n","\n","optimizer=torch.optim.Adam(model.parameters(),lr=lr,weight_decay=weight_d)\n","\n","scheduler=lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min= 0, last_epoch= -1, verbose=True)"],"id":"9ff785ed"},{"cell_type":"code","execution_count":null,"metadata":{"id":"751972a1","scrolled":false},"outputs":[],"source":["#### This line is for the training of the network ####\n","num_epochs = 2\n","mdl, optim, lossTr, accuracy, lossTe=TrainingPipeLine(model, optimizer, scheduler,triplet_loss, training, testing, num_epochs, KNN)\n"],"id":"751972a1"},{"cell_type":"markdown","metadata":{"id":"39165f93"},"source":["## Save and load a model\n","\n","* In the following cell is possible to save the trained model, and eventually to load it."],"id":"39165f93"},{"cell_type":"code","execution_count":64,"metadata":{"id":"241a938b","executionInfo":{"status":"ok","timestamp":1652963815724,"user_tz":-120,"elapsed":242,"user":{"displayName":"andrea giuseppe di francesco","userId":"08400322680190058161"}}},"outputs":[],"source":["path=\"models/three_layerSAGENEW.pt\" ## Change the path to save the trained model where you wish.\n","\n","#### This line calls the function to save the previous obtained results (from the training of the model) ####\n","Save_Model(path,mdl,optim,lossTr,accuracy,lossTe)\n","device = 'cpu'\n","\n","#### This line can be uncommented in order to get data from one of our pre-trained model ####\n","# model_check=Load_Model(\"models/conf1.pt\", device)\n","# print(model_check[\"Accuracy_History\"])"],"id":"241a938b"},{"cell_type":"markdown","source":["# Model results on\n","* Loss (Train/Test)\n","* Accuracy (Test)\n"],"metadata":{"id":"TXHTUI9vGJ6b"},"id":"TXHTUI9vGJ6b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6acd877e"},"outputs":[],"source":["''' Models directories:\n","\n","      Paper inspired architectures:\n","      One_layer_SAGE: \"models/one_layerSAGE.pt\"\n","      Two layers SAGE: \"models/two_layerSAGE.pt\"\n","      Three layers SAGE: \"models/three_layersSAGE.pt\"\n","\n","      Laude task architectures (previously described and defined):\n","      conf1: \"models/conf1.pt\"\n","      conf2: \"models/conf2.pt\"\n","      conf3: \"models/conf3.pt\"\n","      conf4: \"models/conf4.pt\" \n","'''\n","device = 'cuda'\n","arch_paths = [\"models/one_layerSAGENEW.pt\", \"models/two_layerSAGENEW.pt\", \"models/conf2NEW.pt\", \"models/conf3NEW.pt\", \"models/conf4NEW.pt\"]\n","arch_names = [\"One layer GraphSAGE\", \"Two layers GraphSAGE\",  \"Second Configuration\", \"Third Configuration\", \"Fourth Configuration\"]\n","for archs in range(len(arch_paths)):\n","  path = arch_paths[archs]\n","  name_arch = arch_names[archs]\n","  model_check=Load_Model(path,device)\n","  train_diz = model_check[\"Loss_trainHistory\"]\n","  test_diz = model_check[\"Loss_testHistory\"]\n","  plot_metrics(train_diz,test_diz,name_arch)\n","  \n","\n","\n","\n"],"id":"6acd877e"},{"cell_type":"markdown","source":["## Accuracy results\n","* Choose the architecture amongst the 7, and see the results.\n","* The obtained results are shown in results, and are plotted in the following cell."],"metadata":{"id":"uikN7uw7q89I"},"id":"uikN7uw7q89I"},{"cell_type":"code","source":["models = [\"SAGE1\",\"SAGE2\", \"SAGE3\",\"conf1\",\"conf2\",\"conf3\",\"conf4\",\"random_conf3\", \"random_SAGE3\"]\n","results = [0.26275006124936195,0.3189057310486129,0.4274988132047857,0.5669277224607767,0.47781317707844545,0.6927919457182362,0.6213292268763897,0.6982752015757381,0.08483794916268922]\n","get_accuracy(\"SAGE3\",device = 'cuda', instance = XX, model_path = 'models/three_layerSAGENEW.pt',n_layers = 3)\n"],"metadata":{"id":"Vj8o4fGyrKQM"},"id":"Vj8o4fGyrKQM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["prefix = 'Graph'\n","fig = go.Figure(data=[\n","    go.Bar(name=prefix+models[0], x = ['One GraphSAGE layer'], y=[results[0]],text = round(results[0],4), textposition='auto'),\n","    go.Bar(name=prefix+models[1], x = ['Two GraphSAGE layers'], y=[results[1]],text = round(results[1],4), textposition='auto'),\n","    go.Bar(name=prefix+models[2], x = ['Three GraphSAGE layers'], y=[results[2]],text = round(results[2],4), textposition='auto'),\n","    go.Bar(name=prefix+models[8], x = ['Three GraphSAGE layers (random_features)'], y=[results[8]],text = round(results[8],4), textposition='auto'),\n","    go.Bar(name=models[3], x = ['Configuration n°1'], y=[results[3]],text = round(results[3],4), textposition='auto'),\n","    go.Bar(name=models[4], x = ['Configuration n°2'], y=[results[4]],text = round(results[4],4), textposition='auto'),\n","    go.Bar(name=models[5], x = ['Configuration n°3'], y=[results[5]],text = round(results[5],4), textposition='auto'),\n","    go.Bar(name=models[7], x = ['Configuration n°3 (random features)'], y=[results[7]],text = round(results[7],4), textposition='auto'),\n","    go.Bar(name=models[6], x = ['Configuration n°4'], y=[results[6]],text = round(results[6],4), textposition='auto')\n","])\n","fig.update_layout(\n","    title='Performances over the different architectures',\n","    xaxis_tickfont_size=14,\n","    yaxis=dict(\n","        title='Normalized Discounted Cumulative Gain',\n","        titlefont_size=16,\n","        tickfont_size=14,\n","    ),\n","    legend=dict(\n","        x=0,\n","        y=1.0,\n","        bgcolor='rgba(255, 255, 255, 0)',\n","        bordercolor='rgba(255, 255, 255, 0)'\n","    ),\n","    \n","    bargap=0.15, # gap between bars of adjacent location coordinates.\n","    bargroupgap=0.1 # gap between bars of the same location coordinate.\n",")\n","fig.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"x7mRyEYmUBdV","executionInfo":{"status":"ok","timestamp":1652964976963,"user_tz":-120,"elapsed":408,"user":{"displayName":"andrea giuseppe di francesco","userId":"08400322680190058161"}},"outputId":"10d82c5a-e4ee-452e-cc4e-7e4d2a4092b2"},"id":"x7mRyEYmUBdV","execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"db59cfcf-5620-4955-8335-5fae2e4e7822\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"db59cfcf-5620-4955-8335-5fae2e4e7822\")) {                    Plotly.newPlot(                        \"db59cfcf-5620-4955-8335-5fae2e4e7822\",                        [{\"name\":\"GraphSAGE1\",\"text\":\"0.2628\",\"textposition\":\"auto\",\"x\":[\"One GraphSAGE layer\"],\"y\":[0.26275006124936195],\"type\":\"bar\"},{\"name\":\"GraphSAGE2\",\"text\":\"0.3189\",\"textposition\":\"auto\",\"x\":[\"Two GraphSAGE layers\"],\"y\":[0.3189057310486129],\"type\":\"bar\"},{\"name\":\"GraphSAGE3\",\"text\":\"0.4275\",\"textposition\":\"auto\",\"x\":[\"Three GraphSAGE layers\"],\"y\":[0.4274988132047857],\"type\":\"bar\"},{\"name\":\"Graphrandom_SAGE3\",\"text\":\"0.0577\",\"textposition\":\"auto\",\"x\":[\"Three GraphSAGE layers (random_features)\"],\"y\":[0.05773172825557706],\"type\":\"bar\"},{\"name\":\"conf1\",\"text\":\"0.5669\",\"textposition\":\"auto\",\"x\":[\"Configuration n\\u00b01\"],\"y\":[0.5669277224607767],\"type\":\"bar\"},{\"name\":\"conf2\",\"text\":\"0.4778\",\"textposition\":\"auto\",\"x\":[\"Configuration n\\u00b02\"],\"y\":[0.47781317707844545],\"type\":\"bar\"},{\"name\":\"conf3\",\"text\":\"0.6928\",\"textposition\":\"auto\",\"x\":[\"Configuration n\\u00b03\"],\"y\":[0.6927919457182362],\"type\":\"bar\"},{\"name\":\"random_conf3\",\"text\":\"0.6983\",\"textposition\":\"auto\",\"x\":[\"Configuration n\\u00b03 (random features)\"],\"y\":[0.6982752015757381],\"type\":\"bar\"},{\"name\":\"conf4\",\"text\":\"0.6213\",\"textposition\":\"auto\",\"x\":[\"Configuration n\\u00b04\"],\"y\":[0.6213292268763897],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"tickfont\":{\"size\":14}},\"yaxis\":{\"title\":{\"text\":\"Normalized Discounted Cumulative Gain\",\"font\":{\"size\":16}},\"tickfont\":{\"size\":14}},\"legend\":{\"x\":0,\"y\":1.0,\"bgcolor\":\"rgba(255, 255, 255, 0)\",\"bordercolor\":\"rgba(255, 255, 255, 0)\"},\"title\":{\"text\":\"Performances over the different architectures\"},\"bargap\":0.15,\"bargroupgap\":0.1},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('db59cfcf-5620-4955-8335-5fae2e4e7822');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"markdown","source":["As was described in the paper's authors GraphSAGE networks lose in accuracy when we are using a random low level features vector. But if we use the same random approach with the GAT-based architecture (conf3), we don't lose any performance in accuracy, we could get even better results. In fact, the adjacency matrix is based more on musicological features, instead of low level features. So we can say that GraphSage layers are not able to generalize the instance vectors as well as the GAT layers."],"metadata":{"id":"tO9B1osddXff"},"id":"tO9B1osddXff"},{"cell_type":"markdown","source":["## Let's get the artists embeddings\n","Now, given one of the 7 configurations we can test how much are plausible (from a musical point of view), its nearest neighbors.\n","* Firstly, we need to know some artists that are from the test set, so we need to look for them. They are identified by all the indices from 10190 to 11260.\n","* Then we get the points in the embedding space of all the dataset.\n","* Then we compute for a certain arbitrary artist its K-nearest neighbors.\n","* To compute the nearest neighbors we use the same function that was used to compute the accuracy. \n","* Remember that the Graph of artists is obtained through the opinion of experts and the attributes of the nodes represents the low level features. "],"metadata":{"id":"m11QuOt-8xqP"},"id":"m11QuOt-8xqP"},{"cell_type":"code","source":["test_list = list(range(10190,11261))\n","for index in test_list:\n","  print(diz_of_artist[str(index)])   #Ringo Starr, Giacomo Puccini, Michael Jackson, Gigi D'Agostino, Snoop Dogg, Alex Britti, Nancy Sinatra, Rod Stewart."],"metadata":{"id":"vuc2r0eWr6X0"},"id":"vuc2r0eWr6X0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["According to this list and based on our musicological knowledge we have chosen to consider the following artists: Ringo Starr, Giacomo Puccini, Michael Jackson, Gigi D'Agostino, Snoop Dogg, Alex Britti, Nancy Sinatra, Rod Stewart.\n"],"metadata":{"id":"5v1_X2dT-DVY"},"id":"5v1_X2dT-DVY"},{"cell_type":"code","execution_count":79,"metadata":{"id":"dd2972f7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652965825678,"user_tz":-120,"elapsed":7911,"user":{"displayName":"andrea giuseppe di francesco","userId":"08400322680190058161"}},"outputId":"ba6dbd15-c67a-4c43-8406-2a00cc5cf319"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The 20 nearest artist in the embedded space for Ringo Starr are:\n","\n","['David Essex', 'Creedence Clearwater Revival', 'The Doobie Brothers', 'Ron Wood', 'Eric Burdon', 'Steve Miller Band', 'Tom Petty', 'Andy Fairweather‐Low', 'John Fogerty', 'Bill Wyman’s Rhythm Kings', 'Dave Edmunds', 'Bob Seger', 'Delaney Bramlett', 'The Kinks', 'Argent', 'Hank Marvin', 'Timothy B. Schmit', 'Elliott Murphy', 'Brian Wilson', 'Fleetwood Mac']\n","\n","The 20 nearest artist in the embedded space for Ringo Starr are:\n","\n","['Jim Dickinson', 'Tom Petty', 'Steve Winwood', 'John Fogerty', 'David Essex', 'Dire Straits', 'Bruce Springsteen', 'Fleetwood Mac', 'Jeff Lynne', 'Tom Cochrane', 'The Kinks', 'Steve Miller Band', 'Eric Burdon', 'The Doobie Brothers', 'Eric Clapton', 'Al Anderson', 'Bill Wyman’s Rhythm Kings', 'Timothy B. Schmit', 'The Move', 'Don Johnson']\n","\n","The 20 nearest artist in the embedded space for Giacomo Puccini are:\n","\n","['Samuel Barber', 'Ned Rorem', 'Cecilia Bartoli', 'Herbert von Karajan', 'Alfredo Kraus', 'José Cura', 'Plácido Domingo', 'Enrico Caruso', 'Frederica von Stade', 'Sir Neville Marriner', 'Beniamino Gigli', 'Kiri Te Kanawa', 'Fritz Wunderlich', 'Ramón Vargas', 'Béla Bartók', 'Antal Doráti', 'Claudio Abbado', 'Bryn Terfel', 'Giuseppe Di Stefano', 'Royal Scottish National Orchestra']\n","\n","The 20 nearest artist in the embedded space for Giacomo Puccini are:\n","\n","['Samuel Barber', 'Ned Rorem', 'Sir Neville Marriner', 'Cecilia Bartoli', 'Academy of St Martin in the Fields', 'Antal Doráti', 'Herbert von Karajan', 'English Chamber Orchestra', 'Kiri Te Kanawa', 'Béla Bartók', 'Plácido Domingo', 'Royal Philharmonic Orchestra', 'Ruth Etting', 'Bryn Terfel', 'Frederica von Stade', 'Philharmonia Orchestra', 'London Symphony Orchestra', 'André Previn', 'Montserrat Caballé', 'Royal Liverpool Philharmonic Orchestra']\n","\n","The 20 nearest artist in the embedded space for Michael Jackson are:\n","\n","['Teena Marie', 'Stephanie Mills', 'Charlie Wilson', 'Billy Ocean', 'Shalamar', 'Mtume', 'Stacy Lattisaw', 'Terence Trent D’Arby', 'The Jacksons', 'Cheryl Lynn', 'Prince', 'Alexander O’Neal', 'Grace Jones', 'Midnight Star', 'Giorgio Moroder', 'Heatwave', 'Lisa Stansfield', 'Rick James', 'Chic', 'Switch']\n","\n","The 20 nearest artist in the embedded space for Michael Jackson are:\n","\n","['Stephanie Mills', 'Charlie Wilson', 'Billy Ocean', 'Teena Marie', 'The Jacksons', 'Atlantic Starr', 'Lenny Williams', 'Alexander O’Neal', 'Mtume', 'Giorgio Moroder', 'Sydney Youngblood', 'Shalamar', 'Luther Vandross', 'Terence Trent D’Arby', 'Cheryl Lynn', 'Rick James', 'Lisa Stansfield', 'Chic', 'Stacy Lattisaw', 'Jon Secada']\n","\n","The 20 nearest artist in the embedded space for Gigi D’Agostino are:\n","\n","['Mylo', 'Safri Duo', 'Alice DeeJay', 'Junkie XL', 'U96', 'Basement Jaxx', 'Bob Sinclar', 'Andrew Weatherall', 'Ultra Naté', 'Faithless', 'Club 69', 'Nightcrawlers', 'Hot Banditoz', 'The Japanese Popstars', 'Keoki', 'Kate Ryan', 'Deep Dish', 'Basshunter', 'Coco Steel & Lovebomb', 'Freemasons']\n","\n","The 20 nearest artist in the embedded space for Gigi D’Agostino are:\n","\n","['Mylo', 'Bob Sinclar', 'Junkie XL', 'Safri Duo', 'Alice DeeJay', 'Basement Jaxx', 'Pulsedriver', 'U96', 'Andrew Weatherall', 'Hot Banditoz', 'Faithless', 'Danzel', 'Vitalic', 'Kosheen', 'Eric Prydz', 'Tiësto', 'Ultra Naté', 'Deep Dish', 'SebastiAn', \"Guns n' Bombs\"]\n","\n","The 20 nearest artist in the embedded space for Snoop Dogg are:\n","\n","['2Pac', 'Warren G', 'Saafir', 'Tha Dogg Pound', 'Kurupt', 'Too $hort', 'Obie Trice', 'Mack 10', 'Luniz', 'Benzino', 'Suga Free', 'Stat Quo', 'Scarface', 'Spice 1', 'Nas', 'MC Breed', 'Devin the Dude', 'DJ Quik', 'Afroman', 'Tony Yayo']\n","\n","The 20 nearest artist in the embedded space for Snoop Dogg are:\n","\n","['2Pac', 'Tha Dogg Pound', 'Mack 10', 'Luniz', 'Benzino', 'Obie Trice', 'Spice 1', 'Too $hort', 'Fler', 'Kurupt', 'Eminem', 'DJ Quik', 'JAY‐Z', 'Afroman', 'Scarface', 'Stat Quo', 'Tony Yayo', 'Clipse', 'Nas', 'Crooked I']\n","\n","The 20 nearest artist in the embedded space for Alex Britti are:\n","\n","['Alex Baroni', 'Paolo Meneguzzi', 'Francesco Renga', 'Eros Ramazzotti', 'Antonello Venditti', 'Manolo García', 'Luca Carboni', 'Alejandro Sanz', 'Chayanne', 'Marco Masini', 'Pino Daniele', 'Franco de Vita', 'Lucio Dalla', 'Francisco Céspedes', 'Presuntos Implicados', 'Francesco De Gregori', 'Cristian Castro', 'Aleks Syntek', 'Giorgia', 'Roger Cicero']\n","\n","The 20 nearest artist in the embedded space for Alex Britti are:\n","\n","['Alex Baroni', 'Francesco Renga', 'Paolo Meneguzzi', 'Eros Ramazzotti', 'Luca Carboni', 'Antonello Venditti', 'Pino Daniele', 'Marco Masini', 'Francesco De Gregori', 'Ligabue', 'Lucio Dalla', 'Aleks Syntek', 'Elio e le Storie Tese', 'Giorgia', 'Juanes', 'Chayanne', 'Pascal Obispo', 'Alejandro Sanz', 'Manolo García', 'Dyango']\n","\n","The 20 nearest artist in the embedded space for Nancy Sinatra are:\n","\n","['Dusty Springfield', 'Cilla Black', 'The 5th Dimension', 'Harpers Bizarre', 'Pat Boone', 'Helen Shapiro', 'Neil Sedaka', 'Captain & Tennille', 'Paul Anka', 'Sonny & Cher', 'Rita Coolidge', 'Bobby Goldsboro', 'Barry Manilow', 'Frankie Valli', 'Bobby Vinton', 'Joe South', 'Frankie Avalon', 'Samantha Sang', 'Adam Faith', 'Jay & the Americans']\n","\n","The 20 nearest artist in the embedded space for Nancy Sinatra are:\n","\n","['Dusty Springfield', 'Cilla Black', 'Harpers Bizarre', 'Helen Shapiro', 'Sonny & Cher', 'The 5th Dimension', 'Bobbie Gentry', 'Paul Anka', 'Neil Sedaka', 'Frankie Valli', 'Pat Boone', 'Bobby Goldsboro', 'Barbara Lewis', 'Captain & Tennille', 'Barry Manilow', 'Joe South', 'Bobby Vinton', 'Lynn Anderson', 'Frankie Avalon', 'Brenda Lee']\n","\n","The 20 nearest artist in the embedded space for Rod Stewart are:\n","\n","['Fleetwood Mac', 'Ringo Starr', 'Jeff Lynne', 'The Doobie Brothers', 'John Fogerty', 'John Lennon', 'Billy Joel', 'Bruce Hornsby', 'Paul McCartney', 'Paul Carrack', 'Daryl Hall & John Oates', 'David Essex', 'Lindsey Buckingham', 'Don Johnson', 'Brian Wilson', 'Dave Mason', 'Steve Perry', 'Roger Waters', 'Steve Miller Band', 'Stevie Nicks']\n","\n","The 20 nearest artist in the embedded space for Rod Stewart are:\n","\n","['Fleetwood Mac', 'Ringo Starr', 'Phil Collins', 'Dire Straits', 'Paul Carrack', 'John Waite', 'Bruce Springsteen', 'Tom Cochrane', 'Stevie Nicks', 'The Doobie Brothers', 'Daryl Hall & John Oates', 'Steve Perry', 'Jim Dickinson', 'Eric Clapton', 'Jeff Lynne', 'Billy Joel', 'Steve Miller Band', 'Don Johnson', 'Jesse ‘Ed’ Davis', 'John Fogerty']\n"]}],"source":["K = 20 # Number of neighbors of that we are going to look for #\n","artist_list = ['Ringo Starr', 'Giacomo Puccini', 'Michael Jackson', \"Gigi D’Agostino\", 'Snoop Dogg','Alex Britti','Nancy Sinatra','Rod Stewart']\n","device = 'cuda'\n","model_name = 'conf3' ## Choose the model ##\n","\n","## Compute the embeddings based on the architecture ##\n","embs = get_embeddings(model_name,device, model_path = \"models/conf3NEW_random.pt\") #, n_layers = 2)\n","\n","\n","\n","## Choose the embedding ##\n","embedding = embs\n","\n","\n","\n","for artist_name in artist_list:\n","  print(\"\\nThe {} nearest artist in the embedded space for {} are:\\n\".format(K,artist_name))\n","  ind = get_nearest_artists(embedding,artist_name,K,art_to_code,diz_of_artist)\n","  print(ind)\n","  "],"id":"dd2972f7"},{"cell_type":"markdown","source":["* If you want to look for embedding by yourself you can change the setting of the next cell."],"metadata":{"id":"filDDR1eDYyf"},"id":"filDDR1eDYyf"},{"cell_type":"code","source":["K = 20 # Number of neighbors of that we are going to look for #\n","device = 'cuda'\n","model_name = 'SAGE2' ## Choose the model ##\n","\n","## Compute the embeddings based on the architecture ##\n","embs = get_embeddings(model_name,device, model_path = \"models/two_layerSAGENEW.pt\", n_layers = 2)\n","\n","## Choose the embedding ##\n","embedding = embs\n","\n","artist_name = 'Eminem'\n","print(\"\\nThe {} nearest artist in the embedded space for {} are:\\n\".format(K,artist_name))\n","ind = get_nearest_artists(embedding,artist_name,K,art_to_code,diz_of_artist)\n","print(ind)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HV-NWMbkDgPq","executionInfo":{"status":"ok","timestamp":1652959437118,"user_tz":-120,"elapsed":16987,"user":{"displayName":"andrea giuseppe di francesco","userId":"08400322680190058161"}},"outputId":"3c437fe6-cafe-4b81-b1de-f374fc7d965d"},"id":"HV-NWMbkDgPq","execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The 20 nearest artist in the embedded space for Eminem are:\n","\n","['Jedi Mind Tricks', 'Mack 10', 'Inspectah Deck', 'Wu‐Tang Clan', 'Freeway', 'D12', 'Ruff Ryders', 'Snoop Dogg', 'Esham', 'Cypress Hill', 'Tha Alkaholiks', 'Styles P', 'Pharoahe Monch', 'Shyheim', 'Ice‐T', 'Ugly Duckling', 'Funkmaster Flex', 'The Notorious B.I.G.', 'Smif‐n‐Wessun', 'Petey Pablo']\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Olga.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}